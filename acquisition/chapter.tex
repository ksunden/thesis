\chapter{Acquistion} \label{cha:acq}

\newcommand\biab{\texttt{bluesky-in-a-box} }
\newcommand\wrightfakes{\texttt{wright-fakes} }
\newcommand\wrightplans{\texttt{wright-plans} }
\newcommand\blueskycmds{\texttt{bluesky-cmds} }
\newcommand\yaqccmds{\texttt{yaqc-cmds}}
\newcommand\yaqcqtpy{\texttt{yaqc-qtpy}}
\newglossaryentry{UUID}{name=UUID, description={Universally Unique Identifier}}

\clearpage

\section{Introduction}

\subsection{PyCMDS: Monolithic Data Acquisition Program}

\newglossaryentry{PyCMDS}{name=PyCMDS, description={A monolithic Python program for collecting CMDS data}}
\newglossaryentry{CMDS}{name=CMDS, description={Coherent Multi-Dimensional Spectroscopy}}

\Gls{PyCMDS} is a program originally created by Dr. Blaise Thompson to perform \gls{CMDS} experiments\cite{}.
Prior to PyCMDS, the Wright Group had been through several iterations of programs perform experiments, primarily written using LabVIEW\cite{}\cite{}.
As the name suggests, PyCMDS was written in Python.
This jump to a standard text based language meant that changes to the software could be shared more easily.
PyCMDS was the first time the Wright Group wrote acquisition software to run on multiple laser systems.
To achieve this, all of the possible known hardware interfaces were included in application code and a series of configuration files were used to specify which hardware was active.

PyCMDS was a monolithic program, incoporating everything from communicating with hardware to orchestrating motor motion, to recording data files, and providing graphical user interfaces.
While ``modularity'' was an idea that occured for PyCMDS, here it meant that internal structures were reusable, as opposed to allowing integrations with external programs.
Parts of PyCMDS were tightly coupled, meaning that deep knowledge of the entire system was required for even seemingly small changes.

PyCMDS introduced the idea of ``Scan Modules'', separatly parameterizable instructions to do different kinds of data acquistitions.
Initially, the goal was for scientists to be able to create their own scan modules for each new kind of experiment.
However, the complexity of actually implementing them, combined with lack of documentation and the raw flexiblity of the ``default'' module, ``SCAN'', led to no one actually implementing many additional modules.
The concept of ``Constants'', expressions which are used to set hardware that is not an axis of the scan but nonetheless needs to be moved for the desired slice, was introduced by the ``SCAN'' module.

While the design of PyCMDS was good enough to get it running two laser systems that are quite similar in nature, if consisting of different parts from different manufacturers, its faults were evident.
Virtual hardware was implemented to allow for development of PyCMDS away from the active instruments, but even then it made some development tasks.
Ultimately, the rigidity of some hardware interfaces led to the creation of \yaq{}, and the first inclusions of \yaq{} in PyCMDS.
Specifically, some of the hardware interface code was not able to run on Windows 10, making upgrading the system impossible without a large effort.
That upgrade path, along with some issues relating to losing motor positions, was the driving factor in the creation of \yaq{}.


\subsection{\texttt{yaqc-cmds}: Steps towards modularity}

\newglossaryentry{yaqc-cmds}{name=yaqc-cmds, description={Pronounced ``Yak C Commands''. A custom program for collecting CMDS data using \yaq{} to communicate with hardware}}

While the transition was not entirely made overnight, upon removing the last of the hardware interface code from the acquisition program, we decided that it deserved a new name, \yaqccmds{}, pronounced "Yak C Commands".
This name comes from the fact that it is a \yaq{} client, the ``yaqc'', and that it instructs, or ``commands'' the instrument to acquire CMDS data, the ``cmds''.
This name represents one of the biggest steps in the modular approach to instrumentation.
\yaq{} plays a central role in the data collection for the program.

To users, the change was fairly minimal: the user interface was largely unchanged and the available acquisitions were the same.
At the same time, the other big change to the program was that the output data format was updated to be the HDF5-based ``WT5'' format from WrightTools, rather than the plain text PyCMDS data format.
This data format change allows for better representation of multidimensional data sets, especially those collected by using array detectors or cameras, while keeping data file sizes manageable.

\yaqccmds{} is a program attempting to be a one stop shop for both engineering tasks like working up initial signal and for actual data collection.
Figure \ref{acq:fig:yaqccmds} shows the main queue panel of the application.
Along the left side of the program is a control panel for the hardware known to the program.
This allows setting of position and associated values whenever no scans are currently acquiring.
In the middle is the queue itself, running top to bottom in order of execution.
On the right, there is a panel that gives controls for the queue, including creating a new queue, running the queue, and enqueueing new items.

\begin{landscape}
\begin{figure}
\includegraphics[width=9in]{"acquisition/images/yaqc-cmds"}
\caption[yaqc-cmds]{
	The main queue window of \yaqccmds{}.
}
\label{acq:fig:yaqcccmds}
\end{figure}
\end{landscape}

In addition to the queue, \yaqccmds{} has advanced menus for each hardware, including the menus that eventually became part of \yaqcqtpy{} for the Attune daemons.
These advanced menus were much more critical prior to using \yaq{}, but were kept for consistency.

Finally, there is a live plot tab which plots individual slices of data as they are collected.
When the queue is idle, a large number at the top displays the most recent reading from a sensor, which is being polled.

While less monolithic than its progenitor, \yaqccmds{} retained some of its faults.
Writing new acquisition modules, while possible, was not well documented and was challenging to do correctly without deep knowledge of the internals of the program.
This meant that most users, even advanced users, had little to no path to actually realize new ideas they had.
There is no mechanism for running scans outside of the application, so developing new acquisition modules necessitated also implenting their user interface, and the systems were deeply integrated together.

The program also required carefully constructed configuration files.
While each section was relatively easy, \yaqccmds{} expected all enabled hardware to be available when the program starts, and would freeze or fail to open if even one instance of the expected hardware was missing or unresponsive.
Users would frequently need to reconfigure and restart the program when hardware configuration changed, such as enabling or disabling a particular sensor.

Ultimately, the flaws of \yaqccmds{} led to the separation of the program into \yaqcqtpy{} for the engineering focused tasks and a new program for the task of data acquisition.
This new program builds on top of an ecosystem provided by the broader scientific community, Bluesky\cite{}.

\clearpage

\section{\texttt{bluesky}: Fully modular data acquisition}

\subsection{Bluesky}
\newglossaryentry{Blueskyproj}{name={Bluesky (Project)}, description={A collaboration primarily consisting of members of National Labs which provides libraries in Python do perform experimental science}}

\newglossaryentry{Blueskylib}{name={Bluesky (Library)}, description={The central Library of the Bluesky Project which provides an engine for specification and orchestration of scientific experiments}}

Bluesky is a project initially created for the National Synchrotron Light Source II at Brookhaven National Lab.
The project has grown and is now a collaboration from multiple facilities and experimental domains.
Bluesky as a project provides a collection of Python libraries with an aim towards standardizing experimental data collection to be useful at a variety of scales, from a single machine instrument all the way to the large synchrotron facilities.

Bluesky provides a set of standards for how to interact with hardware, and how to format data collection.
It provides libraries for interacting with hardware, and for handling the data that is collected.
At the center is the Bluesky library itself, which provides the orchestration of experiments.


The libraries provided by the Bluesky project are often developed together, providing integration where needed.
However, the libraries are still separable, allowing for alternatives to be incorporated with ease to suit particular needs of an instrument.
For instance, while the Bluesky Project provides Ophyd\cite{}, a useful hardware abstraction library with a particular focus on EPICS process values, the Wright Group has provided an alternative hardware interface based on \yaq{}, \texttt{yaqc-bluesky}\cite{}.

\subsubsection{The Event Model}

The Event Model\cite{} is Bluesky's mechanism for representing collected data.
Each experiment is broken down into a sequence of JSON-compatible objects with particular structures.
Figure \ref{acq:fig:event_model} demonstrats pictorally the four most useful Event Model document types.
An experiment, or ``run'', begins with a ``start'' document.
The start document contains metadata which pertains to the entire experiement.
This includes things such as the parameters of the requested scan, the expected number of points, and user provided information.
Each run is give a \Gls{UUID} which provides a unique key which serves as a name for that particular instance of data collection.
A data stream then begins with a ``descriptor'' document.
The descriptor document provides a set of variables collected together as a ``row'' of data.
The descriptor provides metadata pertaining to each individual value collected such as the source of the variable, the shape of the data, the data type, and its units.
The descriptor contains a reference to the run start identifier, and also provides its own \Gls{UUID} and human readable name.
The actual recorded data points then are provided by ``event'' documents, which provide timestamps and values for each key listed by its associated descriptor document.
Finally, a ``stop'' document closes a run, referencing the UUID of the start document, providing an exit status, error message for failed scans, and information about the actual number of collected events (which may differ from the expected number from the start document).

These four basic events are sufficient to represent the data from most experiments.
The Event Model does provide some additional kinds of documents for specialized tasks such as referencing data that is stored elsewhere (which is useful for large array data) and for bundling multiple events into one document.
However, the details of those document classes are not important for understanding how Bluesky works and records data.


\begin{figure}
\includegraphics[width=5.5in]{"acquisition/images/event_model"}
\caption[The Event Model]{
This figure demonstrates some common usage patterns of the event model.
A ``run'' is opened with a start document and closed with a ``stop'' document.
Data streams are initiated with a ``descriptor'' document and populated with ``event'' documents.
Multiple streams can be included in a single run.
This figure was created by the Bluesky project and is reproduced with permission.
}
\label{acq:fig:event_model}
\end{figure}



\subsubsection{The Run Engine}

The Run Engine is the beating drum of a Bluesky experiment.
At its core, the Run Engine is a loop that consumes ``Messages''.
Messages are instructions which the Run Engine knows how to process.
Some messages cause interactions with hardware, such as setting motor positions or initiating data collection, while other messages output Event Model documents to a series of callback functions.
The callback functions which subscribe to the events can be used to do a variety of tasks from storing data into a file on disk, to printing a table to the console, to plotting data as it is collected.

\subsubsection{Plans}

\newglossaryentry{plan}{name=plan, description={A Python function which defines a series of steps of an experiemental procedure}}

\Glspl{plan} are the source of messages to the run engine.
A plan is a Python generator which yields a series of Message objects.
Plans are parameterized to make common acquisitions easy to specify.
The Bluesky Python library comes with a set of pre-assembled plans which allow specification of a variety of trajectories through motor space and collecting data from sensors at each point.
While the built in plans are sufficient for many experiments, there are often either additional functionalities or better parameterizations for domain-specific tasks which require specialized plans.

\subsection{\wrightplans}

\newglossaryentry{wrightplans}{name={\wrightplans}, description={Bluesky Plans built for the Wright Group}, sort={wright plans}}
% TODO figures demonstrating motorspace for scans

Broadly, \gls{wrightplans} provides wrappers of the standard Bluesky built-in plans which add unit awareness and the idea of ``constants'' allowing setting of motors to algebraic expressions of other motors.
These plans each bear the name of the underlying Bluesky plan with the suffix ``\texttt{\_wp}'' which simply stands for \wrightplans and is included to differentiate from the built-in plans.
Additionally, \wrightplans provides specialized plans for performing tuning operations of OPAs.

\subsubsection{Common parameters to multiple plans}
\label{common_params}

Many of the plans specified in \wrightplans share common behavior and meanings assigned to parameters.
Rather than repeating the details as it pertains to each plan, this section provides how each of these are specified and used.

\paragraph{Detectors}

In all of the plans provided by \wrightplans, the first argument is a list of detectors, i.e. a list of Bluesky protocol complient objects that implement the Readable protocol.
These devices are read at every data point and recorded into the resultant data event stream.
This may include cameras or array detectors which produce channels that have additional axes in the resultant data.

\paragraph{Units support}

Many of the standard built-in Bluesky plans create linearly spaced arrays in the native units for each device.
This proves limiting especially in the case of spectroscopy because different subfields often primarily work with different unit systems; A materials scientist often thinks in eV, while a molecular spectroscopist focusing on vibrational modes mostly thinks in wavenumbers ($cm^{-1}$).
Making it easy for scientists to think in the more natural unit system for their experiment was a priority.
This is even more critical because the units for energy and the units for wavelength are inversely proportional, so a linear spaced array of points in one unit is not the same as a linearly spaced array in an alternative unit.


\newglossaryentry{varcyc}{name={variadic cycle}, description={A repeating cycle of parameters which are logically grouped in sets}} 

A common pattern in the built-in Bluesky plans is to use a ``\gls{varcyc}'' a repeating pattern of three to six arguments passed positionally where each set defines one axis of the scan.
Where this pattern was used in the underlying plan, \wrightplans adds the units as a string to the end of that cycle of arguments.
The exception to this is \texttt{scan\_nd\_wp}, which is specified as a single \texttt{cycler}\cite{} object to define the positions.
In this case, the units are specified as a dictionary mapping the device object to the unit string.
If the unit is not provided in the dictionary or the unit is given as \texttt{None}, then the native units of the device in question are used.

\paragraph{Constants}

Constants allow setting of additional hardware during the course of a scan according to algebraic relationships.
Specifically, Constants allow for positions to be set to linear combinations of other hardware positions.
The simplest case of a Constant is one which is actually Constant throughout the scan.
For example, setting a monochromator to zero nanometers or a delay to one picosecond delay.
Another common example would be scanning two motors simultaneously, as in setting Delay 2 equal to the value of Delay 1.
The most complicated common usage of Constants is to track the phase matching condition of the experiment.
For example, setting a monochromator to the sum of three light sources for a nonlinear spectroscopy experiment.

Constants are also unit aware, with the constraint that each term of the expression have compatible units, meaning that the units can be converted to one another.
The algebra is performed on values in the Constant's specified units, so different resulting positions will occur for a Constant calculated in nanometers versus a Constant with an identical expression but units of wavenumbers.

Constants can be constructed using Python objects provided by \wrightplans, as a list of \texttt{(motor, units, terms)} tuples, or as a dictionary mapping motors to \texttt{(units, terms)} tuples.
The terms themselves can be created as a list of \texttt{ConstantTerm} objects or as a list of \texttt{(coefficient, variable)} tuples, with the variable being \mintedinline{python}{None} for the constant term and a motor for a variable term.
The motors specified by constant terms are not required to be motors otherwise specified by the plan, though if they are not, the motor positions specified by the plan are sensitive to motor positions at the start of the scan.
It is required that there are no cycles for the dependency relationships in constants (i.e. m0 cannot be set to be 1*m1 while m1 is set to 2*m0, as that is an undefined position).

\paragraph{Per Step}

Per Step is a tool that can be used by advanced users to modify the behavior of a plan.
Most standard users can safely ignore Per Step.
It is provided in part because modifying Per Step is the mechanism by which \wrightplans inserts the unit awareness and constant behavior, thus providing the same tool to downstream plan authors who may wish to build plans with additional behavior.
Per Step is actually itself a Bluesky plan which takes the list of detectors, the motor positions specified for that step, and a cache of prior motor positions.
Per Step then actually requests the motor motion and the measurement from the detectors.

\paragraph{\texttt{md}}
\texttt{md} is a dictionary of additional metadata provided for the plan.
The plans themselves add metadata relating to the plan, but this can be overridden using the \texttt{md} keyword argument.
The expectation of \texttt{md} is that the contents will be encoded to JSON.

\subsubsection{\texttt{scan\_wp}}

Scan performs a one dimensional acquisition of linearly spaced points.
If multiple motors are specified, each provides a start and stop position as well as their units, but there is only one keyword argument, \texttt{num}, which sets the number of points in the scan.
This is refered to as the ``inner product'' of the two lists.
Constants are allowed for scan, though the built-in functionality makes them largely superfluous.
It can, however, be easier to think in terms of either end points or expressions for different experiments.
Constants also have requirements regarding units that are not required for motors specified as endpoints.

The signature for \texttt{scan\_wp} is:

\begin{codefragment}{python}
def scan_wp(detectors, *args, num=None, constants=None, per_step=None, md=None):
\end{codefragment}

Here, \texttt{args} is a variadic cycle of four values: A motor (Bluesky Setable), a start position, a stop position (both typed as appropriate for the the motor, commonly floating point numbers), and a string specifying the units.
\texttt{num} specifies the number of points.
It is technically allowed, though discouraged, to pass the number of points positionally (i.e. as the last element of \texttt{args}), though explicitly passing using the keyword is preferred.
\texttt{detectors}, \texttt{constants}, \texttt{per\_step} and \texttt{md} are all as described above in \ref{common_params}.

\begin{codefragment}{python}
# a simple scan
scan0 = scan_wp([det], m0, 0, 1, "mm", num=11)
# a scan with multiple motors specified
scan1 = scan_wp([det], m0, 0, 1, "mm", m1, 2, 3, "mm", num=11)
# equivalent scan using constant of "1*m0 + 1.0" instead of additional axis
scan2 = scan_wp([det], m0, 0, 1, "mm", num=11,
                constants=[[m1, "mm", [(1, m0), (1.0, None)]]])
\end{codefragment}

\subsubsection{\texttt{list\_scan\_wp}}

List Scan is nearly equivalent to Scan, except that instead of specifying end points, an iterable list of positions is provided instead of endpoints and number of points.
Since List Scan provides each of the points that are scanned, there is no requirement that points are linearly spaced.
However, there is a requirement that each of the provided lists are the same length, as it is still doing an inner product.
This makes it a useful plan for logarithmically spaced points spanning multiple orders of magnitude\footnote{The built-in plans actually do provide a \texttt{log\_scan} plan which acts as \texttt{scan}, where the endpoints and number of points are specified, however that is de-facto deprecated in favor of \texttt{list\_scan\_wp}, so it was not wrapped. The latter plan additionally allows similar ease to do symmetric log scans about zero.}.
Similarly, Constants are provided as an option, though they are technically subsumed by the built-in functionality of a List Scan.

The signature for \texttt{list\_scan\_wp} is:

\begin{codefragment}{python}
def list_scan_wp(detectors, *args, constants=None, per_step=None, md=None):
\end{codefragment}

Here, \texttt{args} is a three member variadic cycle consisting of a motor, a list of points (as opposed to the start and stop above), and a string specifying units for each motor.
\texttt{detectors}, \texttt{constants}, \texttt{per\_step} and \texttt{md} are all as described above in \ref{common_params}.

\begin{codefragment}{python}
# a simple list scan, equivalent to a "normal" scan
scan0 = list_scan_wp([det], m0, np.linspace(0, 1, 11), "mm")
# a list scan with non-linearly spaced points
scan1 = list_scan_wp([det], m0, [0, 1,  2, 5, 10], "mm")
# a list scan with multiple motors specified, one pseudolog spaced
# and one linearly spaced
scan2 = list_scan_wp([det], m0, [0, 1, 2, 5, 10], "mm",
	                    m1, [0, 1, 2, 3, 4], "mm")
\end{codefragment}

\subsubsection{\texttt{grid\_scan\_wp}}

Grid Scan is the ``outer product'' counterpart to Scan.
This plan provides a way of doing N-Dimensional scans, where each specified axis rather than scanning together scan against one another.
Grid Scan is the workhorse plan for many data acquisition tasks.
Similar to Scan, the axes are specified as endpoints plus number of points, however each axis now has its own number of points instead of having a single number for all motors.
Constants are needed for Grid Scan because the built-in version does not have any mechanism to specify that two motors should move together rather than against each other.
Additionally, Constants provide a way to specify that some condition, such as that the monochromator tracks the sum of two or three light sources.

The signature for \texttt{grid\_scan\_wp} is:

\begin{codefragment}{python}
def grid_scan_wp(
    detectors, *args, constants=None, snake_axes=False, per_step=None, md=None
):
\end{codefragment}

\texttt{detectors}, \texttt{constants}, \texttt{per\_step} and \texttt{md} are all as described above in \ref{common_params}.

Here, \texttt{args} is a five member variadic cycle consisting of: a motor, start position, stop position, number of points, and units.
\texttt{snake\_axes} allows the user to specify if the trajectory of the scan proceeds to the start position after reaching the end of the slice (the default, False) or winds back and forth, reversing the order that points are collected for each slice (True).
Even finer control of snaking behavior is available by passing a list of motors (which must be motors specified in the scan), which will cause only those axes to snake rather than all axes.
The outermost (first) motor must not be included in that list, as it only ever is set to each position once, and thus has no opportunity to ``snake''
A Grid Scan with only one axis is equivalent to Scan, and the addition of constants means that most desired acquisitions can be accomplished with Grid Scan.

\begin{codefragment}{python}
# a simple grid scan, equivalent to the simple scan above
scan0 = grid_scan_wp([det], m0, 0, 1, 11, "mm")
# a 2D scan
scan1 = grid_scan_wp([det], m0, 0, 1, 11, "mm",
                            m1, 2, 3, 21, "mm")
# scan representing a triple sum frequency experiment
# the constant is m3=m0+m1+m2, evaluated in wavenumbers
scan2 = grid_scan_wp([det], m0, 15000, 20000, "wn", 51,
                            m1, 1500, 2000, 21, "wn",
                            m2, 1500, 2000, 21, "wn",
                            constants=[[m3, "wn", [(1, m0), (1, m1), (1, m2)]]])
\end{codefragment}

\subsubsection{\texttt{list\_grid\_scan\_wp}}

List Grid Scan is to Grid Scan what List Scan is to Scan: rather than specifying endpoints and number of points, lists are provided for each axis.
Similarly, this allows for scanning motor positions which are not linearly spaced, such as logarithmically spaced points.
Since this is an outer product, ther is no requirement that the axes have the same number of points, and since the lists have lengths, there is no explicit requirement to specify the length at all.
Constants provide the same benefits to List Grid Scan as they do to Grid Scan.

The signature for \texttt{list\_grid\_scan\_wp} is:

\begin{codefragment}{python}
def list_grid_scan_wp(
    detectors, *args, constants=None, snake_axes=False, per_step=None, md=None
):
\end{codefragment}

Here, \texttt{args} is a three member variadic cycle: a motor, a list of positions, and the units.
\texttt{detectors}, \texttt{constants}, \texttt{per\_step} and \texttt{md} are all as described above in \ref{common_params}.

\begin{codefragment}{python}
# a list grid scan with multiple motors specified, one pseudolog spaced and one linearly spaced
scan0 = list_grid_scan_wp([det], m0, [0, 1, 2, 5, 10], "mm",
                                 m1, [0, 1, 2], "mm")
\end{codefragment}

\subsubsection{\texttt{scan\_nd\_wp}}

Scan ND is the mechanism by which all of the previous plans actually work.
It is more complicated to use, and thus is rarely directly called, but it is also able to perform more unique scans than are provided by the parameterizations for more standard scans.
It is highly recommended to consider if an equivalent acquisition could be done using one of the above plans prior to reaching for Scan ND; The reparameterized plans provide gaurdrails which make it harder to be incorrect or to get unexpected results.
In fact, the behavior of Constants is also technically not required for Scan ND, though complicated examples involving multiple terms and unit conversion would be difficult to compute for a native Scan ND.

The signature for \texttt{scan\_nd\_wp} is:

\begin{codefragment}{python}
def scan_nd_wp(
    detectors, cycler, *, axis_units=None, constants=None, per_step=None, md=None
):
\end{codefragment}

Instead of specifying motor motion independently to the plan, this is done prior and a single \texttt{cycler}\cite{} object is handed to the Scan ND plan.
This cycler can perform arbitrary inner and outer product computations to define a trajectory through N-dimensional motor space.
As such, it can be used to perform scans similar to at least simple versions of Constants, mixing axes that scan together with those that scan against each other.
The motors are the keys of the Cycler object.
Since there is no ``variadic cycle'' for Scan ND, \wrightplans unit support is provided via a separate keyword argument, \texttt{axis\_units}, a dictionary mapping the motors as keys to strings, the unit to use for that motor.
\texttt{detectors}, \texttt{constants}, \texttt{per\_step} and \texttt{md} are all as described above in \ref{common_params}.

\begin{codefragment}{python}
from cycler import cycler
cy = cycler(motor1, [1, 2, 3]) * cycler(motor2, [4, 5, 6])
scan_nd([sensor], cy, axis_units={motor1: "ps", motor2: "mm"})
\end{codefragment}


\subsubsection{\texttt{motortune}}

The remaining plans that are provided by \wrightplans enable specialized tuning routines designed for Optical Parametric Amplifiers (OPAs).
The first, \texttt{motortune} is a flexible generic plan which can do a wide variety of tasks related to tuning OPAs.
The remaining four plans are reparameterizations of \texttt{motortune} which make common acquisitions easier and more directly specified.

The signature for \texttt{motortune} is:

\begin{codefragment}{python}
def motortune(detectors, opa, use_tune_points, motors, spectrometer=None, *, md=None):
\end{codefragment}

\texttt{detectors} and \texttt{md} are as described above in \ref{common_params}.
\texttt{opa} is the Bluesky device specifying an OPA.
It is expected to have subdevices for the motors specified in \texttt{motors}.
\texttt{use\_tune\_points} is a boolean which if true inserts an axis for the OPA as the outermost (slowest) axis of the scan at the points specified by the tuning curve.
If motors are specified, then only the points specified by the relevant tunes are used, otherwise all points are used.
\texttt{motors} is a dictionary mapping the \textit{names} of the motors (as strings representing the attribute of the \texttt{opa} object) to a dictionary of parameters for each motor.
The motor parameters consist of the following:
\begin{itemize}
	\item \texttt{method}: Must be the string ``static'' or ``scan'' the former meaning that the motor should be held constant for the duration of the scan, the latter meaning that the motor is an axis in the scan.
	\item \texttt{center}: Specifies the position for method ``static'' or the center of the axis for method ``scan'' ignored for method ``scan'' if \texttt{use\_tune\_points} is true, at which point center is relative to the tune point motor position.
	\item \texttt{width}: The width, in native motor units, to scan for method ``scan''.
	\item \texttt{npts}: The number of points for the motor axis in method ``scan''.
\end{itemize}
\texttt{spectrometer} is a dictionary of parameters which control a monochromator.
\texttt{spectrometer} is optional as many tuning operations do not require spectral resolution or detectors are not mounted on a monochromator.
Its keys consist of the following:
\begin{itemize}
	\item \texttt{device}: The Bluesky device representing the spectrometer.
	\item \texttt{method}: Must be the string ``static'', ``zero'', ``track'', ``set'', ``scan'', or ``none''.
		\begin{itemize}
			\item ``static'' means that the monochromator should be set to a specific position at the beginning of the scan, as specified by ``center''.
			\item ``zero'' is equivalent to static, except that the position is always zero nanometers, provided as a convenience.
			\item ``track'' introduces a Constant which sets the spectrometer setpoint to be equal to the OPA's setpoint.
			\item ``set'' is equivalent to ``static'' for acquisitions where \texttt{use\_tune\_points} is false and ``track'' where \texttt{use\_tune\_points} is true.
			\item ``scan'' introduces an axis for the monochromator to be scanned.
			\item ``none'' is an alternate way of specifying no spectrometer at all.
		\end{itemize}
	\item \texttt{center}: Specifies the position for method ``static'' or the center of the axis for method ``scan'', ignored for method ``scan'' if \texttt{use\_tune\_points} is true, at which point center is relative to the tune point.
	\item \texttt{width}: The width, in native motor units, to scan for method ``scan''.
	\item \texttt{npts}: The number of points for the motor axis in method ``scan''.
	\item \texttt{units}: specifies the units for method ``static'' or method ``scan'', if not provided, ``nm'' is assumed.
\end{itemize}

\begin{codefragment}{python}
\noop# Equivalent to an intensity scan
motortune(
    [det],
    w1,
    True,
    {"delay_2":{"method": "scan", "width": 2.0, "npts": 31}},
    {"device": mono, "method": "zero"}
)
# Equivalent to a setpoint scan
motortune(
    [det],
    w1,
    True,
    {"crystal_2":{"method": "scan", "width": 30, "npts": 31}},
    {"device": mono, "method": "scan", "width": -250, "npts": 51, "units": "wn"}
)
# Equivalent to a tune test scan
motortune([det],
    w1,
    True,
    {},
    {"device": mono, "method": "scan", "width": -250, "npts": 51, "units": "wn"}
)
# Equivalent to a holistic scan
motortune([array],
    w1,
    True,
    {"delay_1":{"method": "scan", "width": 1.0, "npts": 31}},
    {"device": mono, "method": "track"}
)

# Pure motorspace scan
motortune([det],
    w1,
    False,
    {"delay_2":{"method": "scan", "width": 2.0, "npts": 31}}
)
\end{codefragment}

\subsubsection{\texttt{run\_intensity}}

\texttt{run\_intensity} is a plan designed to acquire data which will be processed with \mintedinline{python}{attune.intensity}.
Since it is known that such acquisitions will have exactly one motor which is scanned, the complexity of the \texttt{motors} argument to \texttt{motortune} is reduced to three required arguments: \texttt{motor}, \texttt{width}, and \texttt{npts}.
The resultant \texttt{motortune} will always have \texttt{use\_tune\_points} set to True, and thus will always be a two dimensional scan (before accounting for detector axes).

The signature for \texttt{run\_intensity} is:

\begin{codefragment}{python}
def run_intensity(detectors, opa, motor, width, npts, spectrometer, *, md=None):
\end{codefragment}

\texttt{spectrometer} is passed directly to \texttt{motortune}, and thus must be specified in the same manner.

\begin{codefragment}{python}
run_intensity(
    [det],
    w1,
    "delay_2",
    2.0,
    31,
    {"device": mono, "method": "zero"}
)
\end{codefragment}

\subsubsection{\texttt{run\_setpoint}}

The actual implementation of \texttt{run\_setpoint} is nearly identical to \texttt{run\_intensity}, as the scans required are exactly the same as far as motors are concerned.
The difference is that \texttt{run\_setpoint} is expected to have a spectral axis, as that is required for processing with \inlinepython{attune.setpoint}.
This spectral axis may originate from either an array detector or by scanning the spectrometer.

The signature for \texttt{run\_setpoint} is:

\begin{codefragment}{python}
def run_setpoint(detectors, opa, motor, width, npts, spectrometer, *, md=None):
\end{codefragment}

\texttt{spectrometer} is passed directly to \texttt{motortune}, and thus must be specified in the same manner.

\begin{codefragment}{python}
run_setpopint(
    [det],
    w1,
    "crystal_2",
    3.0,
    31,
    {"device": mono, "method": "scan", "width": -250, "npts": 51, "units": "wn"}
)
\end{codefragment}

\subsubsection{\texttt{run\_tune\_test}}

\texttt{run\_tune\_test} is the simplest of the tuning plans, with no motors specified.
It is designed to be processed by \inlinepython{attune.tune\_test}, and thus a spectral axis is expected and as with \texttt{run\_setpoint} may originate from either an array detector or scanning a spectrometer.

The signature for \texttt{run\_tune\_test} is:

\begin{codefragment}{python}
def run_tune_test(detectors, opa, spectrometer, *, md=None):
\end{codefragment}

\texttt{spectrometer} is passed directly to \texttt{motortune}, and thus must be specified in the same manner.

\begin{codefragment}{python}
run_tune_test(
    [det],
    w1,
    {"device": mono, "method": "scan", "width": -250, "npts": 51, "units": "wn"}
)
\end{codefragment}

\subsubsection{\texttt{run\_holistic}}

Finally, \texttt{run\_holistic} defines an acquisition designed to be processed with \inlinepython{attune.holistic}.
Here, one of the motors, \texttt{motor0} is not actually scanned, but rather inferred from the tune points themselves.
It is specified so that the metadata can reflect the logical axes of the scan.
The second motor, \texttt{motor1}, is specified in the same manner as the other derived tuning plans: as a name, width, and number of points.

The signature for \texttt{run\_holistic} is:

\begin{codefragment}{python}
def run_holistic(detectors, opa, motor0, motor1, width, npts, spectrometer, *, md=None):
\end{codefragment}

\texttt{spectrometer} is passed directly to \texttt{motortune}, and thus must be specified in the same manner.

\begin{codefragment}{python}
run_holistic(
    [array],
    w1,
    "crystal_1",
    "delay_1",
    1.0,
    31,
    {"device": mono, "method": "track"}
)
\end{codefragment}


\subsection{\texttt{bluesky-in-a-box}: Background Services to Collect Scientific Data}

\texttt{bluesky-in-a-box} runs all of the services using Docker\cite{}.
Specifically, \texttt{bluesky-in-a-box} uses Docker Compose\cite{} to simultaneously start and stop all services, accounting for dependencies between them.
A summary of the the architecture of \biab{} is provided in Figure \ref{acq:fig:biab_arch}.
There are six docker containers: Redis, Mongo, WT5, zmq-proxy, hw-proxy and re-manager.
These all transmit data to one another, and with additional programs running outside of Docker, as signified by the arrows between boxes.
A summary of the exposed TCP Ports is provided in Table \ref{acq:tab:biab_ports}.

\begin{figure}
\includegraphics[width=7in]{"acquisition/images/bluesky-in-a-box-architecture"}
\caption[\texttt{bluesky-in-a-box} architecture]{
A summary of the parts of the \texttt{bluesky-in-a-box} and the flow of data between them.
The parts above the line are all run inside of docker, and can be brought online and offline simultaneously.
The parts below the line are run on the host system and represent the more direct hardware access and the graphical programs to used to interact with the system.
}
\label{acq:fig:biab_arch}
\end{figure}

\begin{table}
\centering
\begin{tabular}{lll}
\hline
Port  & Protocol & Content    \\ \hline
5567  & zmq      & Event Model Documents publishing   \\
5568  & zmq      & Event Model Documents subscribing    \\
6379  & redis    & Normally only used internally, storage of the queue and history    \\
27017 & mongo    & Event Model Documents storage and access via databroker \\
60615 & JSON/zmq & re-manager control and query    \\
60620 & JSON/zmq & hw-proxy queries    \\
60625 & JSON/zmq & re-manager log entry subscribing    \\ \hline
\end{tabular}
	\caption[\biab{} Ports]{Summary of open ports in \biab{}.
\end{table}

\subsubsection{Installation}
\label{biab-install}

\paragraph{Set up hardware virtualization}

Docker is a technology that allows running services in a controlled environment that is isolated from the programs on the host machine.
To do this, it takes advantage of hardware options on modern Central Processing Units (CPUs).
The hardware options are available on most CPUs, but are often turned off by default and must be turned on in the BIOS settings.
The name and location of this setting will depend on the make and model of the hardware, but often are identified by options with names like ``KVM'' or ``Intel Virtualization Technology''.
Technically, this should only be required for Windows and MacOS installations.

\paragraph{Set up WSL (Windows only)}

The docker containers for \texttt{bluesky-in-a-box} define a Linux\cite{} environment for each service.
Microsoft has invested in the Windows Subsystem for Linux (WSL)\cite{} to make running Linux environments on Windows machines efficient.
To install on Windows 10 or above, you simply need to open a command prompt as an administrator (cmd.exe or powershell) and run \texttt{wsl --install}.
The install process will prompt you to reboot the machine.
It is recommended to install a version of Linux into WSL explicitly, by running \texttt{wsl --install -d debian} (or whatever distribution is your preference).

\paragraph{Install Docker}

On Windows and MacOS, the preferred method of installing Docker is to install Docker Desktop\cite{}.
This is a standard installer wizard which will guide you through the steps to install.
Once installed, you should toggle a handful of settings.
Most notably, under the ``Resources $\rightarrow$ WSL Integration'', it should be enabled for all distributions installed.
Additionally ``Docker Compose V2'' should be enabled.
It is also recommended to consider your personal preference and purpose of using docker to determine if starting docker at login is desirable, if you want weekly tips for using Docker, or if you wish to send usage stats to Docker.
Note that Docker Desktop's license terms require a paid subscription for commercial use, however academic use continues to be free.

On Linux, mostly Docker is installed via the system package manager.
For a Debian or Ubuntu based system this would be:

\begin{codefragment}{bash}
sudo apt install docker.io
sudo apt install docker-compose
\end{codefragment}

It is recommended to add your user account to the \texttt{docker} group to avoid needing to use \texttt{sudo} to perform docker operations:

\begin{codefragment}{bash}
sudo usermod -aG docker $USER
\end{codefragment}

\paragraph{Configure available \yaq{} hardware}

This section assumes that the system has \yaq{} hardware running and set up to be listed by \texttt{yaqd list}.
If this is a development machine, it is recommended to install \ref{wright-fakes} prior to proceeding.

\texttt{bluesky-in-a-box} uses \texttt{happi}\cite{}, a project created by the Stanford Linear Accelerator Center (SLAC), to communicate what hardware is available to the \texttt{re-manager} and \texttt{hwproxy}.
It provides a standard way of declaring available hardware and how to create a Bluesky-compatible Python object for each.
Using \texttt{happi} allows the possibility of easily and transparently using hardware supported by additional control layers beyond \texttt{yaq}.

\texttt{happi} is a Python package that can be installed with either \texttt{conda install happi} or \texttt{pip install happi}.
This will install a Command Line Interface (CLI) Application to manage a database (stored as a JSON file) of available hardware.
In addition, for \texttt{happi} to be able to work with \yaq{} hardware you must \texttt{conda install yaqc-bluesky} or \texttt{pip install yaqc-bluesky}.

To get started, set up a configuration file for \texttt{happi}.
On Windows, start by creating a folder \nolinkurl{~\\AppData\\Local\\happi\\happi}
Inside of this folder, create a file, \nolinkurl{happi.ini} with the following contents (replacing \texttt{<USERNAME>} with the username that is appropriate for your system):

\begin{codefragment}{ini}
\[[DEFAULT]
path = C:\Users\<USERNAME>\AppData\Local\happi\happi\happidb.json
\end{codefragment}

Follow this by running the following in a command prompt:

\begin{codefragment}{bash}
setx /s %COMPUTERNAME% /u %USERNAME% HAPPI_CFG %LOCALAPPDATA%/happi/happi/happi.ini
\end{codefragment}

This sets up an environment variable which is persistent between sessions that tells happi where to load and save changes.

The Linux equivalent would be to create \nolinkurl{~/.config/happi.ini} with the contents of path pointing to \nolinkurl{~/.local/share/happi/happidb.json}.

Once set up, you can add all \texttt{yaq} hardware by doing:

\begin{codefragment}{bash}
yaqd list --format happi | happi update -
\end{codefragment}

This exports all of the daemons that are recognized by \texttt{yaqd-control} into the happi database.
If hardware is added to the system (or information like the host or port are updated) then the same line will update happi again.

\paragraph{Configure data folder}

You should create a data folder where \texttt{bluesky-in-a-box} will place the generated \texttt{wt5} files.
The exact location does not particular matter, but \nolinkurl{~/bluesky-cmds-data} is what will be used in this document.

\paragraph{Download \texttt{bluesky-in-a-box} Docker configuration}

Clone the git repository for \biab:

\begin{codefragment}{bash}
git clone https://github.com/wright-group/bluesky-in-a-box
\end{codefragment}

There are a few options for the Docker Compose files, a template for the file is provided in the root of the \texttt{bluesky-in-a-box} repository.
Copy the template file, \nolinkurl{.env-example} to \nolinkurl{.env}, then edit it.
\texttt{HAPPI\_DB\_PATH} should be set to the full path of \nolinkurl{happidb.json} as specified in \nolinkurl{happi.ini} file above.
\texttt{WT5\_DATA\_PATH} should be set to the full path of the data folder specified above.
Finally, \texttt{TZ} should be set to the appropriate timezone where it is running, for example \texttt{America/Chicago}.


\paragraph{Start and update containers}
Start the docker engine, either by opening Docker Desktop (Windows) or by enabling the background docker service (Linux).
Navigate to the \biab folder in a command prompt.
If needed, pull new versions or switch to appropriate branches that contain the version of \biab that you need.
To build for the first time or to perform a standard update, run the following:

\begin{codefragment}{bash}
docker compose up --build
\end{codefragment}

That will suffice for most cases, however if you want to force a full rebuild, you can use:

\begin{codefragment}{bash}
docker compose build --no-cache
\end{codefragment}

This is most useful as a development tool to ensure that you are not accidentally relying on the state of your local system in ways that would not work on other machines.

Once the containers are built, you can start them using Docker Desktop by pressing the start button for \biab in the ``Containers/Apps'' tab.
This will start the pre-built containers without rebuilding them.
Rebuilding using the CLI is needed whenever the containers themselves are changed, most commonly for bumping the versions of installed software.

% TODO longer term maintenance of docker containers and volumes

\subsubsection{\texttt{re-manager}}

The RE Manager is the beating heart of the \biab system.
It is the service which accepts parameterized plan descriptions, organizes them into a queue, allows manipulation of the queue itself, and, most importantly, actually conducts the experiment.
The RE Manager starts a Bluesky Run Engine (hence the name) and consumes the plans from the queue.
These plans encode the steps for taking data: moving motors, triggering sensors, and emitting Event Model documents.

The RE Manager is responsible for connecting to the devices which are listed by the HAPPI database, providing its own list of available devices (which filters out those that are offline or are subdevices of other hardware such as OPA motors).
At present, \biab expects that all hardware specified is \yaq{} hardware, though support for additional interface layers requires only that they are specified in HAPPI and that the necessary dependencies get added to the \texttt{re-manager} container.
It is also responsible for providing a list of available plans.
For \biab, all of the \wrightplans are available, as well as the built-in Bluesky \texttt{count}, \texttt{sleep}, and \texttt{mv} plans.
\texttt{count} generates a time series of readings, separated by a wait period.
\texttt{sleep} simply waits for a specified period of time, given in seconds.
\texttt{mv} allows for setting of hardware (to positions specified in their native units) in a manner which can be enqueued.
All of the plans which generate data are wrapped with the Bluesky \texttt{baseline} decorator, which reads \textit{all} movable hardware once before and once after the plan itself is consumed.
This allows for a full record of the state of the system without recording all hardware for every data point, even when it not told to move for a given plan.

The RE Manager container provides a few ways of interacting, which are largely based on intefaces over a ZeroMQ\cite{} message system.
ZeroMQ (abbreviated ZMQ or 0MQ) provides a standard way of either doing request/reply Remote Procedure Calls (RPCs) or publish/subscribe (pub/sub) data flows where one end provides a stream of data and the other is expecting that stream of data.
The former, RPC style requests, is used to enqueue plans, interact with the queue itself, and to query the state of the RE Manager.
The latter, pub/sub style requests, is used to publish the Event Model documents as well as (separately) publishing the console output of the RE Manager so that it can be displayed in client applications.
These ZMQ interfaces are considered trusted interfaces, and as such do no authentication of users.
If such authentication is needed, it can be provided by additional tools by the authors of \texttt{bluesky-queueserver} which wrap the ZMQ interface in a FastAPI\cite{} interface served over TCP, with authentication done by that service.
This is recommended in the future if remote access is desired to instruments runing \biab, however initial development is focused on running a self contained system which provides access primarily to the direct machine running the instrument.

\subsubsection{\texttt{zmq-proxy}}

\texttt{zmq-proxy} is a small program provide by Bluesky which simply routes Event Model documents that it reads from on port and writes them to any number of subscribers on a second port.
The same purpose in larger systems where caching, latency, and network reliability are important may be achived by an industry standard service like Apache Kafka\cite{}.
Because \biab is running on a single machine, network reliability and latency are not anticipated problem and we do not currently require caching, so the simpler program is used.
\texttt{zmq-proxy} provides a server for each end of the communication, which allows multiple clients to act as sources or sinks of the documents which are being shuttled.
In practice, usually only one service, the RE Manager, should ordinarily be the source of the documents.
However, this does provide an easy way for testing or special purpose scripts to serve as a source.

\subsubsection{\texttt{WT5}}

The WT5 container consumes the Event Model Document stream from the \texttt{zmq-proxy} container and creates WrightTools Data files.
These files are stored in a folder shared with the host platform, such that they can be browsed and opened using platform native tools.
In addition, the WT5 container writes a number of text files into the data folder and generates images for many scans.
These include the ``start'' ``descriptor'' and ``stop'' documents from the event model for each Bluesky Run, written as JSON text files.
Additionally each generated WT5 file also writes the output of \texttt{data.print\_tree(verbose=True)} to a file so that users can quickly see information such as the shape and axes of the data file without actually opening the file.

One and Two dimensional scans produce the ``quick'' plots provided by WrightTools.
Larger scans have caused significant memory problems when attempting to plot many slices, and so have been disabled.

\subsubsection{\texttt{mongo}}

MongoDB\cite{} is an industry standard database for storing JSON-like documents.
Mongo is widely used by the upstream authors of Bluesky at various National Labs as the immediate store of data being recorded from instruments.
As such, there are pre-existing tools in the ecosystem that pair well with a Mongo database.
The Mongo Docker container is provided to allow exploration of this ecosystem of tools with the data collected by \biab.
It also provides a back-up of the data should the WT5 data writer fail.
At present, the Mongo database does not serve as a long term archival data back-up, but rather as a a short term failsafe.
This decision is not one of fundemental design, but rather a decision of allocation of development time.
It is quick and easy to add a Mongo container to the Docker system and route the data from the RE Manager to it, ensuring it is accessible short term.
It is a more complex task to ensure that the data gets written in a manner appropriate for archival storage, and there has been no need for it at this time.

\subsubsection{\texttt{redis}}

Redis\cite{} is an industry standard service for in-memory key-value data store.
It is used by the RE Manager container to track the state of the queue of plans.
Redis is required for the operation of the RE Manager, and thus must be running.
However, there is little to no direct interaction with redis, so unless you are actively developing \texttt{bluesky-queueserver}, it can largely be ignored.

\subsubsection{\texttt{hwproxy}}

\texttt{bluesky-hwproxy}\cite{} is a small project started for inclusion in \biab.
Its primary purpose is to provide read-only access to the hardware which is available to the RE Manager.
This allows for rich graphical programs like \blueskycmds to query information about hardware such as limits and units.
\texttt{hwproxy} wraps the device loading portions of \texttt{bluesky-queueserver} and exposes the standard Bluesky Protocol methods over a ZMQ interface very similar to that used by the RE Manager itself.
This allows any tool that can communicate with the RE Manager to communicate with \texttt{hwproxy} just as easily.

\subsubsection{Troubleshooting}

Docker as a technology for running \biab was chosen primarily to make standard usage as easy as possible.
It allows for all of the connected services to be brought online and offline quickly and easily, encoding all of their dependent relationships on each other.
However, the same factors that make Docker a good choice for standard operation mean that a few extra steps are needed when troubleshooting or debugging.
In particular, Docker is designed to provide a consistent environment for all of the services to run, with static code and dependencies.
Troubleshooting is a task which necessitates rapid updating and a tight feedback loop.

While the standard troubleshooting tips as laid out in \ref{TODO} apply, there are some particular approaches that are useful when debugging \biab in particular (and Docker containers more generally).
The first step is to determine \textit{where} the origin of the problem exists.
This is not as trivial as it may seem at first.
Generally, it is a good strategy to start by carefully reading the logs provided by the Docker containers.
The two most relevant containers will be the the RE Manager and the WT5 containers.
The other containers are all fairly standard tools that are not nearly as customized and thus are not likely to be the problem.
Most of the time, the container which has an error message is a good candidate for where the problem exists, but this is not always the case.
Some problems may exist in the network itself which connects the containers to each other and to the host system.
These tend to be relatively rare, but can also be some of the trickiest to track down and resolve.
Others may present as errors down stream but the root cause is the input to that container.
For example, faulty parameters may cause a plan to fail during execution.
Such a failure may be expected, given the inputs, so this example means that either user training or user interfaces could be improved.
Alternatively, if the metadata is malformed that may allow the actual data acquisition to proceed as expected, but cause the WT5 data writer to fail.
In this instance, the root cause would be the Bluesky plan, which is a dependency of the RE Manager, though the error would present in the WT5 container.


For smaller tests, particularly those involving testing of the network connections, Docker allows opening a command shell which runs inside of the container.
If using Docker Desktop, starting a command shell inside of one of the Docker containers is as easy as pressing a button in the graphical application.
On any platform, you can use the Docker command line to open a shell in the Docker container:

\begin{codefragment}{bash}
docker exec -it <container name> bash
\end{codefragment}

Note that the container name here is as listed by \inlinebash{docker ps}, not the name in the Docker Compose file.
For \biab, this is most useful for information gathering tasks such as answering the question "Can the RE Manager container communicate with the yaq daemon running on the host machine?" or "Can my WT5 container communicate with the ZMQ Proxy container?".

If the problem exists with the code that is in the \biab repository (mostly the startup scripts, configuration, and the WT5 data writing code) then a more traditional editing the code and running the containers is reasonably achievable.
There is the required step to do \inlinebash{docker compose up --build} to run the Docker containers.
Without doing this, the old version will run instead.
This makes for a reasonable, if a little slower than ideal, development feedback loop.

If, instead, the problem exists in your dependencies, that makes developing even more indirect.
There is not a particularly good way to build the Docker containers pointing to local directories with code changes.
Thus the procedure is instead to push the code changes to a remotely accessible Git repository and install Python dependencies from that Git repository.
Note, however, that while you \textit{can} point the Git installation in \texttt{requirements.txt} to the branch that you create, this is not fully recommended because subsequent updates to the branch will not cause the Docker container to rebuild.
Docker will only redo a step in the installation if it can tell that the step requires it.
Thus it will only redo the step that installs the Python dependencies if it sees a change to what it is asked to install.
Since the requested branch is the same, the Docker build system determines that it is the same version being requested and thus will use the previously built version.
Instead, it is recommended to use the SHA hash pointing to a particular commit instead, and updating it every time the code is pushed to the remote repository.

An example of a \texttt{requirements.txt} that points to a Git repository:

\begin{codefragment}{python}
bluesky==1.8.3
#WrightTools==3.4.4
pyzmq==22.3.0
numpy==1.22.3
toolz==0.11.2
git+https://github.com/wright-group/WrightTools@abcdef12
\end{codefragment}

The last line adds a particular Git version of \wrighttools which takes the place the commented out version specified earlier in the requirements file.

Finally, and perhaps most powerfully, the modularity of the Docker system can be used to run parts of the system outside of docker itself, thus enabling more traditional development and feedback cycles.
Any of the Docker containers can be independently halted and superceded by a version of the service running outside of Docker, though clients of that service may need to be reconfigured to point to the correct external service.
While it can be used if there are problems discovered with the MongoDB, Redis, or ZMQ Proxy contianers, for \biab this is most useful to perform tests using a smaller Run Engine running outside of the RE Manager container all together.
\ref{apx:script:re} provides an example of a script that can be run directly on the host machine, performing data acquisitions which are still processed by the WT5 data writing pipeline (via the ZMQ Proxy container).
This particular script does not require \textit{any} change to the configuration of \biab.
In fact, even the RE Manager container can be running, though it should not be actively acquiring data, as downstream tools such as the WT5 writer and live plotting are likely to behave poorly if there are two simultaneous acquisitions happening.
This script allows for a much smaller and easier to troubleshoot system to do the actual orchestration of the acquisition.
It removes some of the complexities of of the RE Manager container, allowing for more direct access to the Run Engine.
This comes, of course, at the cost of not having a queue or the user interfaces built to interact with the RE Manager.

% Slow and methodical.

% Do not be afraid to try things; take precautions such as physically blocking laser light if needed

% Examine and think carefully about expected results; do parameters make sense

% Seek a reproducible test case

% Find cases which work as expected as well as those which fail unexpectedly


\subsection{\texttt{qserver}: Command Line Frontend to \texttt{bluesky-queueserver}}

\texttt{qserver} is a command line program for interacting with \texttt{bluesky-queueserver}.
It is provided by the project itself, and serves as a valueable tool for both troubleshooting and as an ``escape hatch'' for things that more narrowly focussed interfaces do not make possible.

\subsubsection{Installation}

\texttt{qserver} is included with the installation of the Python package \texttt{bluesky-queueserver}.
As such it is installed via:

\begin{codefragment}{bash}
conda install bluesky-queueserver
\end{codefragment}

or:

\begin{codefragment}{bash}
pip install bluesky-queueserver
\end{codefragment}

\subsubsection{Usage}

The most common use for \texttt{qserver} is to simply query the server for its current state:

\begin{codefragment}{bash}
$ qserver status
Arguments: ['status']
17:29:47 - MESSAGE: 
{'devices_allowed_uid': '60b8e736-2b3e-4531-831f-d46dec9a477b',
 'devices_existing_uid': 'f203f778-5e9d-454d-9c8d-30a3c193c8f9',
 'items_in_history': 19,
 'items_in_queue': 0,
 'lock': {'environment': False, 'queue': False},
 'lock_info_uid': '581a37ac-d2d9-4967-b998-d044f1570d25',
 'manager_state': 'idle',
 'msg': 'RE Manager v0.0.16',
 'pause_pending': False,
 'plan_history_uid': '587f5b9f-ffe0-404b-be11-37b723c628ad',
 'plan_queue_mode': {'loop': False},
 'plan_queue_uid': '19b7952c-9c2f-4bd5-a621-6d5750912916',
 'plans_allowed_uid': '0650be18-94d3-4966-969c-a0b1264a2473',
 'plans_existing_uid': '08851d8e-8156-4407-9f60-6d7e7979bc10',
 'queue_stop_pending': False,
 're_state': 'idle',
 'run_list_uid': '2a9e07dc-f8c7-4c39-a9b6-c90f6c9b9833',
 'running_item_uid': None,
 'task_results_uid': 'efc52454-d902-4358-9470-80de6a7416db',
 'worker_background_tasks': 0,
 'worker_environment_exists': True,
 'worker_environment_state': 'idle'}
\end{codefragment}

This gives a fair amount of helpful information, such as the size of the queue and history, and the state of various parts of the queue server itself.

Additional examples of how to use \texttt{qserver} are provided by the built-in help text:

\begin{codefragment}{bash}
$ qserver help
\end{codefragment}

This will print an extensive list of available uses for the program, including adding items to the queue and controlling the running state of the queue.
All of the interactions that are possible as a client of the QueueServer are possible through this interface, though many of them are easier to interact with in a more specialized interface such as the graphical one detailed in the next section.

\subsection{\blueskycmds: Graphical Frontend to \texttt{bluesky-queueserver}}

\blueskycmds is a graphical application to interact with a running Bluesky Queueserver, such as that provided by \biab.
While everything \textit{can} be accomplished using the command line tool above, that is not the most ergonomic tool for users.
The CLI requires that users type out exactly the correct command, including properly formatting JSON where it is required.
As a graphical app, \blueskycmds encodes much of the tedious structure of requests to the Queueserver in the user interface itself, thus minimizing potential for simple typographical errors.

\blueskycmds inherits much of its style, layout, and capabilities from the prior graphical data acquisition apps, \yaqccmds and PyCMDS.
It does, however, remove all of the direct interaction with hardware that the older programs had in order to focus solely on being the best application to manipulate the RE Manager queue.
This functionality is provided by alternative applications such as \yaqcqtpy.
Additionally, the actual code to perform acquisitions and store data is no longer part of the same application, replaced by requests of the Queueserver.
This means that the graphical app can be closed (intentionally or unintentionally) and the acquisistion continues.

\subsubsection{Installation}

\blueskycmds is included with the installation of the Python package \blueskycmds.
However, the package is not yet available on conda, so conda systems should install the dependencies via conda first.
As such it is installed via:

\begin{codefragment}{bash}
conda install bluesky-queueserver-api appdirs click pyqtgraph pyside2 qtpy toml toolz wrighttools sympy bluesky-widgets
pip install bluesky-hwproxy bluesky-cmds --no-deps
\end{codefragment}

or:

\begin{codefragment}{bash}
pip install bluesky-cmds
\end{codefragment}

or, for development purposes (conda users should still install dependencies as above):

\begin{codefragment}{bash}
git clone https://github.com/wright-group/bluesky-cmds
cd bluesky-cmds
flit install --pth-file
\end{codefragment}

\subsubsection{Configuration}

If running locally, then the default configuration options should suffice.
To populate or edit the configuration file you can execute:

\begin{codefragment}{bash}
bluesky-cmds edit-config
\end{codefragment}

This will prompt you to populate the file with the default template if it does not already exist, and open the configuration file in a text editor.

The default template, appropriate for connecting to a \biab instance running on the local machine is shown below:

\begin{codefragment}{toml}\noop[bluesky]
re-manager = "tcp://localhost:60615"
re-info = "tcp://localhost:60625"
hwproxy = "tcp://localhost:60620"
zmq-proxy = "localhost:5568"
\end{codefragment}

To run remotely, the host and port for each connected service can change as needed.

\subsubsection{Usage}


Starting \blueskycmds is done by running the following command:

\begin{codefragment}{bash}
bluesky-cmds
\end{codefragment}

In the future, integrations with the operating systems (Windows, Linux, MacOS) will allow \blueskycmds to appear as an ordinary application that can be started from application menus.

When the application opens, it communicates with the RE Manager, retrieving lists of available devices and plans.
Additionally the application opens the Run Engine worker environment.
The startup behavior is repeated whenever the application reconnects to the RE Manager (e.g. when the RE Manager is restarted)

The application window has a progress bar accross the top, and a tab view which provides the main user interaction.
The tab view has three tabs: Queue, Plot, and Logs.
The progress bar gives an indication of the progress of a running acquisition.
For the progress bar to work, the expected number of points must be known at the begining of the scan and the application must be open when the acquisition begins.
The progress bar provides an elapsed time counter on the left hand side and an estimation of remaining time on the right hand side.
The bar is green when the acquisition starts or completes successfully, and turns red if the aquisition fails.

\subsubsection{The Queue Tab}

The Queue tab is the default tab when the application starts up.
Figure \ref{acq:fig:queue_tab} shows the Queue tab.
Along the left hand side, there is a pane to control the queue, allowing enqueing of plans and starting the queue.
On the right, there is a table which displays the queue and the history.

\begin{landscape}
\begin{figure}
\includegraphics[width=9in]{"acquisition/images/queue_tab"}
\caption[The Queue Tab]{
	\blueskycmds with the queue tab open.
}
\label{acq:fig:queue_tab}
\end{figure}
\end{landscape}

To begin an enqueued scan, click the teal ``Start Queue'' button.
While the queue is running, this button turns into a red ``Interrupt'' button.
When pressed, the RE Manager will pause execution of the plan and a dialog window presents options of how to proceed: ``Resume'', ``Stop after plan'', ``Skip'', and ``Stop now''.
Figure \ref{acq:fig:queue_interrupt} shows the interruption workflow.
``Resume'' will cause the paused plan to proceed from where it left off.
``Stop after plan'' will instruct the queue to resume the current plan, but not proceed to the next enqueued plan.
``Skip'' instructs the RE Manager to halt execution of the current plan, marking the plan as successful and not re-enqueuing it.
``Stop Now'' instructs the RE Manager to halt execution of the current plan, but mark the plan as unsuccessful and re-enqueue it to run again.

\begin{figure}
\includegraphics[width=3in]{"acquisition/images/queue_interrupted"}
\caption[The Interrupt Workflow]{
	When an acquisition is in progress, the Queue Start button becomes an Interrupt button which pauses the acquisition and presents options for how to proceed.
}
\label{acq:fig:queue_interrupt}
\end{figure}

The queue and history can be cleared by clicking the appropriate button in the left hand pane of \blueskycmds.
Each of these will open a confirmation dialog as shown in Figure \ref{acq:fig:queue_clear} to prevent accidentally removing enqueued or recently run plans.
Doing so is rarely absolutely necessary, though it is easier than removing enqueued items individually and the user interface can be slower than desired if there are many entries in the queue and history.

\begin{figure}
\includegraphics[width=3in]{"acquisition/images/queue_clear"}
\caption[The Queue and History Clear Workflow]{
	The buttons for clearing the queue and the history and the pop-up windows that open for each.
}
\label{acq:fig:queue_clear}
\end{figure}

Below the queue and history clearing controls, there is a drop down box with three options: ``plan'', ``instruction'', and ``preset''.
This controls the remainder of the left hand bar to allow adding various items to the queue.

\paragraph{Enqueueing a plan}

The primary way to enqueue a plan in \blueskycmds is to use the ``plan'' option of the ``Type'' selector.
The next selector provided is the ``Plan'' selector, which provides a list of allowed plans.
Each plan then has its own interface for setting the arguments to that specific plan.
These interfaces are currently collated by hand and provided by \blueskycmds, though in the future may be able to be built automatically by information provide by the RE Manager.
There is a fall-back option if no custom interface is provided where JSON arguments and keyword arguments can be provided directly.
Figure \ref{acq:fig:grid_scan} shows the interface for the \texttt{grid\_scan\_wp} plan.

There are common patterns used in building these interfaces.
For instance, all of the plans with variadic cycles have buttons to add and remove sets of inputs that must be provided together.
There are also reusable interface elements for providing metadata, detectors, or constants.

Plans are appended by clicking the teal button at the bottom of the plan interface.

\begin{figure}
\includegraphics[width=3in]{"acquisition/images/grid_scan"}
\caption[Example Plan Enqueuing UI]{
	The user interface for enqueueing a grid scan plan.
}
\label{acq:fig:grid_scan}
\end{figure}

\paragraph{Enqueueing an instruction}
Instructions are items that can be enqueued that modify the behavior of the queue itself.
At present, there is only a single option for instructions to append to the queue: Queue Stop.
There are no parameters for Queue Stop, so it appears as a simple button which when pressed enqueues a Queue Stop at the end of the active queue.
This instructs the queue to return to an idle state rather than proceeding to the next enqueued plan, requiring the user to explicitly restart the queue to continue.
This can be useful if there is some manual user interaction required, but the next experimenal steps are known so the plans can be enqueued before that interaction can occur.
Figure \ref{acq:fig:instruction} shows the instruction pane in the user interface.


\begin{figure}
\includegraphics[width=3in]{"acquisition/images/instruction"}
\caption[Instruction Pane]{
	The ``instruction'' pane for appending to the queue.
}
\label{acq:fig:instruction}
\end{figure}

\paragraph{Enqueueing a preset}

A ``preset'' is simply an ordered list of plans (or instructions) which can saved and enqueued sequentially.
This is useful if there are repetative tasks such as tuning operations or running the same battery of scans on different samples.
Under the hood, presets are files stored in a configuration folder where the file name identifies the preset name and the contents are JSON objects, separated by lines, which represent a series of plans or instructions.
The preset system is functionality added by \blueskycmds, it is not built-in to the RE Manager directly.
When a preset is selected to be appended to the queue, \blueskycmds opens the preset file and reads each line and appends each entry to the queue.

Figure \ref{acq:fig:preset} shows the preset enqueueing pane.
The second drop-down box allows selecting one of the existing preset options.
The teal ``Append Preset Plans'' button enqueues all of the plans from the preset.
The yellow ``Edit Preset Plans'' button opens a text editor to edit the JSON file directly.
This is an advanced feature, but provides the power and flexibility to those who need it.
While it can be used to completely add new entries to the preset, it is most useful for smaller tweaks.
Changing the value of parameters, order of enqueuing, or removing plans from presets are all relatively simple tasks in the text editor view.

An example of a preset file:

\begin{codefragment}{json}
\noop
{"item_type": "plan", "name": "count", "args": [["daq"], 3]}
{"item_type": "plan", "name": "count", "args": [["daq"]], "kwargs": {"num": 5, "delay": 0}, "meta": {"Name": "", "Info": "", "Experimentor": "Kyle"}}
{"item_type": "plan", "name": "count", "args": [["daq"], 5]}
{"item_type": "plan", "name": "mv", "args": ["d0", 0, "d1", 0]}
{"item_type": "instruction", "name": "queue_stop"}

\end{codefragment}

There is presently no built in user interface to remove a preset entirely.
To do so, you must delete the file from the folder of presets, which is located in \nolinkurl{~/.local/share/bluesky-cmds/presets} on Linux and \nolinkurl{\~\\AppData\\Local\\bluesky-cmds\\bluesky-cmds\\presets} on Windows.


\begin{figure}
\includegraphics[width=3in]{"acquisition/images/preset"}
\caption[Preset Pane]{
	The ``preset'' pane for appending to the queue.
}
\label{acq:fig:preset}
\end{figure}

To add to a preset, right click on the description field of an entry in the queue or history, and select ``Append to preset...''.
This will open a dialog box with a selector for existing presets, which if selected the plan will be appended to the end of the preset.
If you select ``New Preset...'', then an additional diagog will open, where a name for a new preset can be provided.
This workflow is shown in Figure \ref{acq:fig:preset_add}.

\begin{figure}
\includegraphics[width=3in]{"acquisition/images/append_to_preset"}
\caption[Preset add]{
	The workflow for appending to presets.
}
\label{acq:fig:preset_add}
\end{figure}

In the main body of the window, a table represents the current state of the queue and history.
More recently enqueued items appear towards the top of the table, older items towards the bottom.
The items waiting in queue, thus, appear at the top, and the items in the history appear at the bottom.
The currently running item (if it exists) appears at the boundary between the queue and the history.
The enqueued and running items have descriptions shown in bright white, while the completed items appear with gray descriptions.
The right most column indicates the position in the queue, given as a zero-based index.
Items can be reordered by editing the value in that column.
On left, there are two columns with buttons: remove and load.
Items in the queue (but not the history) can be removed individually with the red ``REMOVE'' buttons.
All items can be loaded into the sidebar where the parameters can be modified and a similar (or identical) item enqueued.
The other information provided include the plan name, the status (enqueued, RUNNING, completed, failed, aborted, stopped or unknown) and the description field, which lists the arguments passed to the plan.
Hovering over the description shows the full JSON of the item (see Figure \ref{acq:fig:hover_text}, including an error message if applicable.
The description has a contextual menu accessed via right clicking which provides additional options as shown in Figure \ref{acq:fig:right_click}.
Most of the items in the context menu simply copy information to the clipboard: The full JSON as shown by the hover text, the item \Gls{UUID} which identifies the item in the queue, and the run \Gls{UUID} which identifies the data that is collected by the scan.

\begin{figure}
\includegraphics[width=5in]{"acquisition/images/hover_text"}
\caption[Queue hover text]{
	The hover text of a queue item.
}
\label{acq:fig:hover_text}
\end{figure}


\begin{figure}
\includegraphics[width=3in]{"acquisition/images/right_click_menu"}
\caption[Right click menu]{
	The right click menu of a queue item.
}
\label{acq:fig:right_click}
\end{figure}

\subsubsection{The Plot Tab}

The Plot tab presents a live representation of recent data collected for the current plan.
The primary window is a PyQtGraph\cite{} plot showing the most recent five slices of collected data.
The current slice is the brightest cyan slice, with the previous slices shown as duller colors progressively until the last one which is gray.
At the top, the currently plotted channel name and most recent collected value are shown in large font size.
The plotting does handle array detectors, for which the top number shown is the maximum value of the most recent array collected.
At present, cameras and other higher dimensional sensors are not able to be plotted using the live plot tab.

On the right hand side, there are controls to adjust the plot.
At the top, a selector allows for choosing which channel to plot.
The second selector determines which values appear along the X-axis of the graph.
In particular, to switch between a scalar channel and an array detected channel, both selectors will need to be adjusted, as the appropriate axes differ.
Additionally, there is a selector which allows for changing the units of the X-axis.
Finally, there is a Scan Index indicator, which indicates which indexed pixel was the most recent collected.

\begin{landscape}
\begin{figure}
\includegraphics[width=9in]{"acquisition/images/plot_tab"}
\caption[The Plot Tab]{
	\blueskycmds with the Plot tab open.
}
\label{acq:fig:log_tab}
\end{figure}
\end{landscape}

\subsubsection{The Logs Tab}

Figure \ref{acq:fig:log_tab} shows the Logs tab of \blueskycmds.
This is a text field which shows log entries from both the local application and the logs emitted from \biab's RE Manager.
Log levels are color coded as are the logging instances which indicate what system is providing the log entry.
At the bottom of the Logs tab, there is a button which clears the contents of the logging text box.
The Logs tab is a good first step towards troubleshooting when something unexpected happens.

\begin{landscape}
\begin{figure}
\includegraphics[width=9in]{"acquisition/images/log_tab"}
\caption[The Log Tab]{
	\blueskycmds with the Logs tab open.
}
\label{acq:fig:log_tab}
\end{figure}
\end{landscape}

\subsection{\texttt{wright-fakes}: Simulated Hardware for Development}
\label{wright-fakes}

\wrightfakes is a collection of docker containers each running a single \yaq{} daemon.
\wrightfakes provides an entire system, mimicking a laser table, but using only fake daemons to represent motors.
It includes detectors, a monochromator, delays, neutral density wheels, and OPAs (including the component motors of the OPAs and delays).

This serves as a quick and easy way to start a suite of \yaq{} daemons which is consistent and provides the variety of hardware needed to thoroughly test the data acquisition software.

\subsubsection{Installation}
These instructions assume that Docker is already installed, for more information see the \ref{biab-install}.


Clone the git repository for \wrightfakes:

\begin{codefragment}{bash}
git clone https://github.com/wright-group/wright-fakes
\end{codefragment}

Inside of this repository there are folders for each available system (at time of writing, this is only \texttt{fs}).
Navigate in a command prompt to the folder for the system you wish to start and run:

\begin{codefragment}{bash}
docker compose up --build
\end{codefragment}

This builds the system of simulated daemons and starts running the daemons through docker.
If the port that is expected for a \yaq{} daemon is already in use, the container will fail to run, much the same as if you tried to run a second instance of a \yaq{} daemon


If you are using Docker Desktop, then you can restart the fake daemons from the ``Containers/Apps'' tab of the Docker Desktop application, in the same manner as \biab.

Once installed, you need to update the \texttt{yaqd-control} cache so that it knows about all of the daemons:

\begin{codefragment}{bash}
yaqd scan
\end{codefragment}

On Windows, this can be a long process, so you may wish to search only the ranges where actual daemons are running

\begin{codefragment}{bash}
yaqd scan --start=38001 --stop=38003
yaqd scan --start=38401 --stop=38403
yaqd scan --start=38451 --stop=38453
yaqd scan --start=38500 --stop=38501
yaqd scan --start=38550 --stop=38551
yaqd scan --start=38999 --stop=39000
yaqd scan --start=39301 --stop=39303
yaqd scan --start=39501 --stop=39508
yaqd scan --start=39601 --stop=39608
yaqd scan --start=39701 --stop=39703
yaqd scan --start=39876 --stop=39877
\end{codefragment}

\subsection{Bluesky Community and Ecosystem}

While an impressive library in its own right, the real power of Bluesky comes from the ecosystem that surrounds the library.
There is a community of people who are all working towards making various pieces of the ecosystem better.
Some are adding hardware interfaces, much like \yaqcbluesky{}, which expand the available hardware that can be used by Bluesky.
The ``default'' such library, Ophyd\cite{}, is currently undergoing a major transformation with the goal of making it easier to adapt to new hardware and interface libraries\cite{NOBUGS_Ophyd_v2}.
While \yaq{} is a key piece of enabling the Wright Group to use Bluesky, the future could include using one of the alternate libraries for some key pieces of hardware, which might not even need any work on the part of Wright Group members themselves, other than configuration.

In addition to hardware, other components of the ecosystem are constanly being improved and worked on, including ones that we do not currently use.
This includes components for building GUIs, Bluesky Widgets, and access to data after it is recorded, Databroker and Tiled.
These three projects, in particular, are worth monitoring for their features and periodically considering if it is worth the time investment to adapt workflows of the Wright Group.

\subsubsection{Bluesky Widgets}
Bluesky Widgets\cite{} is a library which provides reusable graphical components for interacting with Bluesky and specifically for Bluesky QueueServer.
In addition to the reusable components, the library provides a set of example applications which show practical versions of the components the library provides.
Bluesky Widgets is actually already a dependency of \blueskycmds{}, though only for its data model for ingesting event documents, and not for any of the Qt widgets which are provided.
Some of the example applications, such as the ``Queue Monitor'' application may prove to be generally useful either along side of or instead of programs like \blueskycmds{}.

\subsubsection{Databroker}
Databroker\cite{} is the ecosystem provided tool for accessing data from Bluesky acquisitions.
Primarily, it returns data as Xarray\cite{} Dataset objects, which can then be manipulated, plotted, and stored for future use.
In particular, Databroker can read data directly from the Mongo database that stores each event in sequence.
This is part of the reason for even having the Mongo database in the first place.
Ultimately, the initial decision was to ease the transition for users by providing WT5 files much like they are already used to.
However, the tools provided by databroker and Xarray may yet prove to easily allow some data manipulation that is hard or impossible with WrightTools and WT5.
Additionally, Databroker is in the process of converting to being built on top of the next project, Tiled, so perhaps it would be better for users to use Tiled directly even if the further tools such as Xarray are desired.

\subsubsection{Tiled}

In many ways, Tiled is the successor to Databroker.
As a project, Tiled is quite ambitious, aiming to provide a unified interface to many different forms of data.
Tiled provides a server, which ingests data from folders or databases and provides metadata about the tree of data provided.
Tiled also provides a client Python library, which includes the ability to search data, download data in a variety of formats, including loading only portions of the available data.
Additionally, there is a web server which provides the ability to browse data and view previews as images or graphs interactively.
Tiled is a powerful tool that is well designed and built on standards.
Tiled was not immediately used by the Wright Group because a) it was actively being developed in parallel,  b) it represents a larger change to user workflow than we wished to introduce in one step, and c) it incurs additional systemic complexity of running an additional server component.
In the future, it may be possible to get a large portion of the benefits of Tiled, with minimal disruption of user workflow, by using plug-ins to allow tiled to read and provide WT5 files directly.


\clearpage
