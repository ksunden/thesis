\chapter{Acquistion} \label{cha:acq}

\newcommand\biab{\texttt{bluesky-in-a-box} }
\newcommand\yaq{\texttt{yaq} }
\newcommand\wrightfakes{\texttt{wright-fakes} }
\newcommand\wrightplans{\texttt{wright-plans} }

\clearpage

\section{Introduction}  % =========================================================================

\subsection{PyCMDS: Monolithic Data Acquisition Program}

\newglossaryentry{PyCMDS}{name=PyCMDS, description={A monolithic python program for collecting CMDS data}}
\newglossaryentry{CMDS}{name=CMDS, description={Coherent Multi-Dimensional Spectroscopy}}

\Gls{PyCMDS} is a program to do \gls{CMDS}

\clearpage

\section{\texttt{yaqc-cmds}: Steps towards modularity}  % =============================================================

\newglossaryentry{yaqc-cmds}{name=yaqc-cmds, description={Pronounced ``Yak C Commands''. A custom program for collecting CMDS data using \yaq to communicate with hardware}}

\section{\texttt{bluesky}: Fully modular data acquisition}  % ===================================================================

\subsection{Bluesky}
\newglossaryentry{Blueskyproj}{name={Bluesky (Project)}, description={A collaboration primarily consisting of members of National Labs which provides libraries in python do perform experimental science}}

\newglossaryentry{Blueskylib}{name={Bluesky (Library)}, description={The central Library of the Bluesky Project which provides an engine for specification and orchestration of scientific experiments}}

\subsubsection{The Event Model}

\subsubsection{The Run Engine}

\subsubsection{Plans}

\newglossaryentry{plan}{name=plan, description={A python function which defines a series of steps of an experiemental procedure}}

\subsection{\wrightplans}

\newglossaryentry{wrightplans}{name={\wrightplans}, description={Bluesky Plans built for the Wright Group}, sort={wright plans}}
% TODO figures demonstrating motorspace for scans

Broadly, \gls{wrightplans} provides wrappers of the standard Bluesky built-in plans which add unit awareness and the idea of ``constants'' allowing setting of motors to algebraic expressions of other motors.
These plans each bear the name of the underlying Bluesky plan with the suffix ``\texttt{\_wp}'' which simply stands for \wrightplans and is included to differentiate from the built-in plans.
Additionally, \wrightplans provides specialized plans for performing tuning operations of OPAs.

\subsubsection{Common parameters to multiple plans}
\label{common_params}

Many of the plans specified in \wrightplans share common behavior and meanings assigned to parameters.
Rather than repeating the details as it pertains to each plan, this section provides how each of these are specified and used.

\paragraph{Detectors}

In all of the plans provided by \wrightplans, the first argument is a list of detectors, i.e. a list of Bluesky protocol complient objects that implement the Readable protocol.
These devices are read at every data point and recorded into the resultant data event stream.
This may include cameras or array detectors which produce channels that have additional axes in the resultant data.

\paragraph{Units support}

Many of the standard built-in Bluesky plans create linearly spaced arrays in the native units for each device.
This proves limiting especially in the case of spectroscopy because different subfields often primarily work with different unit systems; A materials scientist often thinks in eV, while a molecular spectroscopist focusing on vibrational modes mostly thinks in wavenumbers ($cm^{-1}$).
Making it easy for scientists to think in the more natural unit system for their experiment was a priority.
This is even more critical because the units for energy and the units for wavelength are inversely proportional, so a linear spaced array of points in one unit is not the same as a linearly spaced array in an alternative unit.


\newglossaryentry{varcyc}{name={variadic cycle}, description={A repeating cycle of parameters which are logically grouped in sets}} 
A common pattern in the built-in Bluesky plans is to use a ``\gls{varcyc}'' a repeating pattern of three to six arguments passed positionally where each set defines one axis of the scan.
Where this pattern was used in the underlying plan, \wrightplans adds the units as a string to the end of that cycle of arguments.
The exception to this is \texttt{scan\_nd\_wp}, which is specified as a single \texttt{cycler}\cite{} object to define the positions.
In this case, the units are specified as a dictionary mapping the device object to the unit string.
If the unit is not provided in the dictionary or the unit is given as \texttt{None}, then the native units of the device in question are used.

\paragraph{Constants}

Constants allow setting of additional hardware during the course of a scan according to algebraic relationships.
Specifically, Constants allow for positions to be set to linear combinations of other hardware positions.
The simplest case of a Constant is one which is actually Constant throughout the scan.
For example, setting a monochromator to zero nanometers or a delay to one picosecond delay.
Another common example would be scanning two motors simultaneously, as in setting Delay 2 equal to the value of Delay 1.
The most complicated common usage of Constants is to track the phase matching condition of the experiment.
For example, setting a monochromator to the sum of three light sources for a nonlinear spectroscopy experiment.

Constants are also unit aware, with the constraint that each term of the expression have compatible units, meaning that the units can be converted to one another.
The algebra is performed on values in the Constant's specified units, so different resulting positions will occur for a Constant calculated in nanometers versus a Constant with an identical expression but units of wavenumbers.

Constants can be constructed using Python objects provided by \wrightplans, as a list of \texttt{(motor, units, terms)} tuples, or as a dictionary mapping motors to \texttt{(units, terms)} tuples.
The terms themselves can be created as a list of \texttt{ConstantTerm} objects or as a list of \texttt{(coefficient, variable)} tuples, with the variable being \mintedinline{python}{None} for the constant term and a motor for a variable term.
The motors specified by constant terms are not required to be motors otherwise specified by the plan, though if they are not, the motor positions specified by the plan are sensitive to motor positions at the start of the scan.
It is required that there are no cycles for the dependency relationships in constants (i.e. m0 cannot be set to be 1*m1 while m1 is set to 2*m0, as that is an undefined position).

\paragraph{Per Step}

Per Step is a tool that can be used by advanced users to modify the behavior of a plan.
Most standard users can safely ignore Per Step.
It is provided in part because modifying Per Step is the mechanism by which \wrightplans inserts the unit awareness and constant behavior, thus providing the same tool to downstream plan authors who may wish to build plans with additional behavior.
Per Step is actually itself a Bluesky plan which takes the list of detectors, the motor positions specified for that step, and a cache of prior motor positions.
Per Step then actually requests the motor motion and the measurement from the detectors.

\paragraph{\texttt{md}}
\texttt{md} is a dictionary of additional metadata provided for the plan.
The plans themselves add metadata relating to the plan, but this can be overridden using the \texttt{md} keyword argument.
The expectation of \texttt{md} is that the contents will be encoded to JSON.

\subsubsection{\texttt{scan\_wp}}

Scan performs a one dimensional acquisition of linearly spaced points.
If multiple motors are specified, each provides a start and stop position as well as their units, but there is only one keyword argument, \texttt{num}, which sets the number of points in the scan.
This is refered to as the ``inner product'' of the two lists.
Constants are allowed for scan, though the built-in functionality makes them largely superfluous.
It can, however, be easier to think in terms of either end points or expressions for different experiments.
Constants also have requirements regarding units that are not required for motors specified as endpoints.

The signature for \texttt{scan\_wp} is:

\begin{codefragment}{python}
def scan_wp(detectors, *args, num=None, constants=None, per_step=None, md=None):
\end{codefragment}

Here, \texttt{args} is a variadic cycle of four values: A motor (Bluesky Setable), a start position, a stop position (both typed as appropriate for the the motor, commonly floating point numbers), and a string specifying the units.
\texttt{num} specifies the number of points.
It is technically allowed, though discouraged, to pass the number of points positionally (i.e. as the last element of \texttt{args}), though explicitly passing using the keyword is preferred.
\texttt{detectors}, \texttt{constants}, \texttt{per\_step} and \texttt{md} are all as described above in \ref{common_params}.

\begin{codefragment}{python}
# a simple scan
scan0 = scan_wp([det], m0, 0, 1, "mm", num=11)
# a scan with multiple motors specified
scan1 = scan_wp([det], m0, 0, 1, "mm", m1, 2, 3, "mm", num=11)
# equivalent scan using constant of "1*m0 + 1.0" instead of additional axis
scan2 = scan_wp([det], m0, 0, 1, "mm", num=11,
                constants=[[m1, "mm", [(1, m0), (1.0, None)]]])
\end{codefragment}

\subsubsection{\texttt{list\_scan\_wp}}

List Scan is nearly equivalent to Scan, except that instead of specifying end points, an iterable list of positions is provided instead of endpoints and number of points.
Since List Scan provides each of the points that are scanned, there is no requirement that points are linearly spaced.
However, there is a requirement that each of the provided lists are the same length, as it is still doing an inner product.
This makes it a useful plan for logarithmically spaced points spanning multiple orders of magnitude\footnote{The built-in plans actually do provide a \texttt{log\_scan} plan which acts as \texttt{scan}, where the endpoints and number of points are specified, however that is de-facto deprecated in favor of \texttt{list\_scan\_wp}, so it was not wrapped. The latter plan additionally allows similar ease to do symmetric log scans about zero.}.
Similarly, Constants are provided as an option, though they are technically subsumed by the built-in functionality of a List Scan.

The signature for \texttt{list\_scan\_wp} is:

\begin{codefragment}{python}
def list_scan_wp(detectors, *args, constants=None, per_step=None, md=None):
\end{codefragment}

Here, \texttt{args} is a three member variadic cycle consisting of a motor, a list of points (as opposed to the start and stop above), and a string specifying units for each motor.
\texttt{detectors}, \texttt{constants}, \texttt{per\_step} and \texttt{md} are all as described above in \ref{common_params}.

\begin{codefragment}{python}
# a simple list scan, equivalent to a "normal" scan
scan0 = list_scan_wp([det], m0, np.linspace(0, 1, 11), "mm")
# a list scan with non-linearly spaced points
scan1 = list_scan_wp([det], m0, [0, 1,  2, 5, 10], "mm")
# a list scan with multiple motors specified, one pseudolog spaced
# and one linearly spaced
scan2 = list_scan_wp([det], m0, [0, 1, 2, 5, 10], "mm",
	                    m1, [0, 1, 2, 3, 4], "mm")
\end{codefragment}

\subsubsection{\texttt{grid\_scan\_wp}}

Grid Scan is the ``outer product'' counterpart to Scan.
This plan provides a way of doing N-Dimensional scans, where each specified axis rather than scanning together scan against one another.
Grid Scan is the workhorse plan for many data acquisition tasks.
Similar to Scan, the axes are specified as endpoints plus number of points, however each axis now has its own number of points instead of having a single number for all motors.
Constants are needed for Grid Scan because the built-in version does not have any mechanism to specify that two motors should move together rather than against each other.
Additionally, Constants provide a way to specify that some condition, such as that the monochromator tracks the sum of two or three light sources.

The signature for \texttt{grid\_scan\_wp} is:

\begin{codefragment}{python}
def grid_scan_wp(
    detectors, *args, constants=None, snake_axes=False, per_step=None, md=None
):
\end{codefragment}

\texttt{detectors}, \texttt{constants}, \texttt{per\_step} and \texttt{md} are all as described above in \ref{common_params}.

Here, \texttt{args} is a five member variadic cycle consisting of: a motor, start position, stop position, number of points, and units.
\texttt{snake\_axes} allows the user to specify if the trajectory of the scan proceeds to the start position after reaching the end of the slice (the default, False) or winds back and forth, reversing the order that points are collected for each slice (True).
Even finer control of snaking behavior is available by passing a list of motors (which must be motors specified in the scan), which will cause only those axes to snake rather than all axes.
The outermost (first) motor must not be included in that list, as it only ever is set to each position once, and thus has no opportunity to ``snake''
A Grid Scan with only one axis is equivalent to Scan, and the addition of constants means that most desired acquisitions can be accomplished with Grid Scan.

\begin{codefragment}{python}
# a simple grid scan, equivalent to the simple scan above
scan0 = grid_scan_wp([det], m0, 0, 1, 11, "mm")
# a 2D scan
scan1 = grid_scan_wp([det], m0, 0, 1, 11, "mm",
                            m1, 2, 3, 21, "mm")
# scan representing a triple sum frequency experiment
# the constant is m3=m0+m1+m2, evaluated in wavenumbers
scan2 = grid_scan_wp([det], m0, 15000, 20000, "wn", 51,
                            m1, 1500, 2000, 21, "wn",
                            m2, 1500, 2000, 21, "wn",
                            constants=[[m3, "wn", [(1, m0), (1, m1), (1, m2)]]])
\end{codefragment}

\subsubsection{\texttt{list\_grid\_scan\_wp}}

List Grid Scan is to Grid Scan what List Scan is to Scan: rather than specifying endpoints and number of points, lists are provided for each axis.
Similarly, this allows for scanning motor positions which are not linearly spaced, such as logarithmically spaced points.
Since this is an outer product, ther is no requirement that the axes have the same number of points, and since the lists have lengths, there is no explicit requirement to specify the length at all.
Constants provide the same benefits to List Grid Scan as they do to Grid Scan.

The signature for \texttt{list\_grid\_scan\_wp} is:

\begin{codefragment}{python}
def list_grid_scan_wp(
    detectors, *args, constants=None, snake_axes=False, per_step=None, md=None
):
\end{codefragment}

Here, \texttt{args} is a three member variadic cycle: a motor, a list of positions, and the units.
\texttt{detectors}, \texttt{constants}, \texttt{per\_step} and \texttt{md} are all as described above in \ref{common_params}.

\begin{codefragment}{python}
# a list grid scan with multiple motors specified, one pseudolog spaced and one linearly spaced
scan0 = list_grid_scan_wp([det], m0, [0, 1, 2, 5, 10], "mm",
                                 m1, [0, 1, 2], "mm")
\end{codefragment}

\subsubsection{\texttt{scan\_nd\_wp}}

Scan ND is the mechanism by which all of the previous plans actually work.
It is more complicated to use, and thus is rarely directly called, but it is also able to perform more unique scans than are provided by the parameterizations for more standard scans.
It is highly recommended to consider if an equivalent acquisition could be done using one of the above plans prior to reaching for Scan ND; The reparameterized plans provide gaurdrails which make it harder to be incorrect or to get unexpected results.
In fact, the behavior of Constants is also technically not required for Scan ND, though complicated examples involving multiple terms and unit conversion would be difficult to compute for a native Scan ND.

The signature for \texttt{scan\_nd\_wp} is:

\begin{codefragment}{python}
def scan_nd_wp(
    detectors, cycler, *, axis_units=None, constants=None, per_step=None, md=None
):
\end{codefragment}

Instead of specifying motor motion independently to the plan, this is done prior and a single \texttt{cycler}\cite{} object is handed to the Scan ND plan.
This cycler can perform arbitrary inner and outer product computations to define a trajectory through N-dimensional motor space.
As such, it can be used to perform scans similar to at least simple versions of Constants, mixing axes that scan together with those that scan against each other.
The motors are the keys of the Cycler object.
Since there is no ``variadic cycle'' for Scan ND, \wrightplans unit support is provided via a separate keyword argument, \texttt{axis\_units}, a dictionary mapping the motors as keys to strings, the unit to use for that motor.
\texttt{detectors}, \texttt{constants}, \texttt{per\_step} and \texttt{md} are all as described above in \ref{common_params}.

\begin{codefragment}{python}
True
\end{codefragment}


\subsubsection{\texttt{motortune}}

The remaining plans that are provided by \wrightplans enable specialized tuning routines designed for Optical Parametric Amplifiers (OPAs).
The first, \texttt{motortune} is a flexible generic plan which can do a wide variety of tasks related to tuning OPAs.
The remaining four plans are reparameterizations of \texttt{motortune} which make common acquisitions easier and more directly specified.

The signature for \texttt{motortune} is:

\begin{codefragment}{python}
def motortune(detectors, opa, use_tune_points, motors, spectrometer=None, *, md=None):
\end{codefragment}

\texttt{detectors} and \texttt{md} are as described above in \ref{common_params}.
\texttt{opa} is the Bluesky device specifying an OPA.
It is expected to have subdevices for the motors specified in \texttt{motors}.
\texttt{use\_tune\_points} is a boolean which if true inserts an axis for the OPA as the outermost (slowest) axis of the scan at the points specified by the tuning curve.
If motors are specified, then only the points specified by the relevant tunes are used, otherwise all points are used.
\texttt{motors} is a dictionary mapping the \textit{names} of the motors (as strings representing the attribute of the \texttt{opa} object) to a dictionary of parameters for each motor.
The motor parameters consist of the following:
\begin{itemize}
	\item \texttt{method}: Must be the string ``static'' or ``scan'' the former meaning that the motor should be held constant for the duration of the scan, the latter meaning that the motor is an axis in the scan.
	\item \texttt{center}: Specifies the position for method ``static'' or the center of the axis for method ``scan'' ignored for method ``scan'' if \texttt{use\_tune\_points} is true, at which point center is relative to the tune point motor position.
	\item \texttt{width}: The width, in native motor units, to scan for method ``scan''.
	\item \texttt{npts}: The number of points for the motor axis in method ``scan''.
\end{itemize}
\texttt{spectrometer} is a dictionary of parameters which control a monochromator.
\texttt{spectrometer} is optional as many tuning operations do not require spectral resolution or detectors are not mounted on a monochromator.
Its keys consist of the following:
\begin{itemize}
	\item \texttt{device}: The Bluesky device representing the spectrometer.
	\item \texttt{method}: Must be the string ``static'', ``zero'', ``track'', ``set'', ``scan'', or ``none''.
		\begin{itemize}
			\item ``static'' means that the monochromator should be set to a specific position at the beginning of the scan, as specified by ``center''.
			\item ``zero'' is equivalent to static, except that the position is always zero nanometers, provided as a convenience.
			\item ``track'' introduces a Constant which sets the spectrometer setpoint to be equal to the OPA's setpoint.
			\item ``set'' is equivalent to ``static'' for acquisitions where \texttt{use\_tune\_points} is false and ``track'' where \texttt{use\_tune\_points} is true.
			\item ``scan'' introduces an axis for the monochromator to be scanned.
			\item ``none'' is an alternate way of specifying no spectrometer at all.
		\end{itemize}
	\item \texttt{center}: Specifies the position for method ``static'' or the center of the axis for method ``scan'', ignored for method ``scan'' if \texttt{use\_tune\_points} is true, at which point center is relative to the tune point.
	\item \texttt{width}: The width, in native motor units, to scan for method ``scan''.
	\item \texttt{npts}: The number of points for the motor axis in method ``scan''.
	\item \texttt{units}: specifies the units for method ``static'' or method ``scan'', if not provided, ``nm'' is assumed.
\end{itemize}

\begin{codefragment}{python}
True
\end{codefragment}

\subsubsection{\texttt{run\_intensity}}

\texttt{run\_intensity} is a plan designed to acquire data which will be processed with \mintedinline{python}{attune.intensity}.
Since it is known that such acquisitions will have exactly one motor which is scanned, the complexity of the \texttt{motors} argument to \texttt{motortune} is reduced to three required arguments: \texttt{motor}, \texttt{width}, and \texttt{npts}.
The resultant \texttt{motortune} will always have \texttt{use\_tune\_points} set to True, and thus will always be a two dimensional scan (before accounting for detector axes).

The signature for \texttt{run\_intensity} is:

\begin{codefragment}{python}
def run_intensity(detectors, opa, motor, width, npts, spectrometer, *, md=None):
\end{codefragment}

\texttt{spectrometer} is passed directly to \texttt{motortune}, and thus must be specified in the same manner.

\begin{codefragment}{python}
True
\end{codefragment}

\subsubsection{\texttt{run\_setpoint}}

The actual implementation of \texttt{run\_setpoint} is nearly identical to \texttt{run\_intensity}, as the scans required are exactly the same as far as motors are concerned.
The difference is that \texttt{run\_setpoint} is expected to have a spectral axis, as that is required for processing with \inlinepython{attune.setpoint}.
This spectral axis may originate from either an array detector or by scanning the spectrometer.

The signature for \texttt{run\_setpoint} is:

\begin{codefragment}{python}
def run_setpoint(detectors, opa, motor, width, npts, spectrometer, *, md=None):
\end{codefragment}

\texttt{spectrometer} is passed directly to \texttt{motortune}, and thus must be specified in the same manner.

\begin{codefragment}{python}
True
\end{codefragment}

\subsubsection{\texttt{run\_tune\_test}}

\texttt{run\_tune\_test} is the simplest of the tuning plans, with no motors specified.
It is designed to be processed by \inlinepython{attune.tune\_test}, and thus a spectral axis is expected and as with \texttt{run\_setpoint} may originate from either an array detector or scanning a spectrometer.

The signature for \texttt{run\_tune\_test} is:

\begin{codefragment}{python}
def run_tune_test(detectors, opa, spectrometer, *, md=None):
\end{codefragment}

\texttt{spectrometer} is passed directly to \texttt{motortune}, and thus must be specified in the same manner.

\begin{codefragment}{python}
True
\end{codefragment}

\subsubsection{\texttt{run\_holistic}}

Finally, \texttt{run\_holistic} defines an acquisition designed to be processed with \inlinepython{attune.holistic}.
Here, one of the motors, \texttt{motor0} is not actually scanned, but rather inferred from the tune points themselves.
It is specified so that the metadata can reflect the logical axes of the scan.
The second motor, \texttt{motor1}, is specified in the same manner as the other derived tuning plans: as a name, width, and number of points.

The signature for \texttt{run\_holistic} is:

\begin{codefragment}{python}
def run_holistic(detectors, opa, motor0, motor1, width, npts, spectrometer, *, md=None):
\end{codefragment}

\texttt{spectrometer} is passed directly to \texttt{motortune}, and thus must be specified in the same manner.

\begin{codefragment}{python}
True
\end{codefragment}


\subsection{\texttt{bluesky-in-a-box}: Background Services to Collect Scientific Data}

\texttt{bluesky-in-a-box} runs all of the services using Docker\cite{}.
Specifically, \texttt{bluesky-in-a-box} uses Docker Compose\cite{} to simultaneously start and stop all services, accounting for dependencies between them.

\begin{figure}
\includegraphics[width=7in]{"acquisition/images/bluesky-in-a-box-architecture"}
\caption[\texttt{bluesky-in-a-box} architecture]{
A summary of the parts of the \texttt{bluesky-in-a-box} and the flow of data between them.
The parts above the line are all run inside of docker, and can be brought online and offline simultaneously.
The parts below the line are run on the host system and represent the more direct hardware access and the graphical programs to used to interact with the system.
}
True
\label{acq:fig:biab_arch}
\end{figure}

\subsubsection{Installation}
\label{biab-install}

\paragraph{Set up hardware virtualization}

Docker is a technology that allows running services in a controlled environment that is isolated from the programs on the host machine.
To do this, it takes advantage of hardware options on modern Central Processing Units (CPUs).
The hardware options are available on most CPUs, but are often turned off by default and must be turned on in the BIOS settings.
The name and location of this setting will depend on the make and model of the hardware, but often are identified by options with names like ``KVM'' or ``Intel Virtualization Technology''.
Technically, this should only be required for Windows and MacOS installations.

\paragraph{Set up WSL (Windows only)}

The docker containers for \texttt{bluesky-in-a-box} define a Linux\cite{} environment for each service.
Microsoft has invested in the Windows Subsystem for Linux (WSL)\cite{} to make running Linux environments on Windows machines efficient.
To install on Windows 10 or above, you simply need to open a command prompt as an administrator (cmd.exe or powershell) and run \texttt{wsl --install}.
The install process will prompt you to reboot the machine.
It is recommended to install a version of Linux into WSL explicitly, by running \texttt{wsl --install -d debian} (or whatever distribution is your preference).

\paragraph{Install Docker}

On Windows and MacOS, the preferred method of installing Docker is to install Docker Desktop\cite{}.
This is a standard installer wizard which will guide you through the steps to install.
Once installed, you should toggle a handful of settings.
Most notably, under the ``Resources $\rightarrow$ WSL Integration'', it should be enabled for all distributions installed.
Additionally ``Docker Compose V2'' should be enabled.
It is also recommended to consider your personal preference and purpose of using docker to determine if starting docker at login is desirable, if you want weekly tips for using Docker, or if you wish to send usage stats to Docker.
Note that Docker Desktop's license terms require a paid subscription for commercial use, however academic use continues to be free.

On Linux, mostly Docker is installed via the system package manager.
For a Debian or Ubuntu based system this would be:

\begin{codefragment}{bash}
sudo apt install docker.io
sudo apt install docker-compose
\end{codefragment}

It is recommended to add your user account to the \texttt{docker} group to avoid needing to use \texttt{sudo} to perform docker operations:

\begin{codefragment}{bash}
sudo usermod -aG docker $USER
\end{codefragment}

\paragraph{Configure available \yaq hardware}

This section assumes that the system has \yaq hardware running and set up to be listed by \texttt{yaqd list}.
If this is a development machine, it is recommended to install \ref{wright-fakes} prior to proceeding.

\texttt{bluesky-in-a-box} uses \texttt{happi}\cite{}, a project created by the Stanford Linear Accelerator Center (SLAC), to communicate what hardware is available to the \texttt{re-manager} and \texttt{hwproxy}.
It provides a standard way of declaring available hardware and how to create a Bluesky-compatible Python object for each.
Using \texttt{happi} allows the possibility of easily and transparently using hardware supported by additional control layers beyond \texttt{yaq}.

\texttt{happi} is a Python package that can be installed with either \texttt{conda install happi} or \texttt{pip install happi}.
This will install a Command Line Interface (CLI) Application to manage a database (stored as a JSON file) of available hardware.
In addition, for \texttt{happi} to be able to work with \yaq hardware you must \texttt{conda install yaqc-bluesky} or \texttt{pip install yaqc-bluesky}.

To get started, set up a configuration file for \texttt{happi}.
On Windows, start by creating a folder \nolinkurl{~\\AppData\\Local\\happi\\happi}
Inside of this folder, create a file, \nolinkurl{happi.ini} with the following contents (replacing \texttt{<USERNAME>} with the username that is appropriate for your system):

\begin{codefragment}{ini}
\[[DEFAULT]
path = C:\Users\<USERNAME>\AppData\Local\happi\happi\happidb.json
\end{codefragment}

Follow this by running the following in a command prompt:

\begin{codefragment}{bash}
setx /s %COMPUTERNAME% /u %USERNAME% HAPPI_CFG %LOCALAPPDATA%/happi/happi/happi.ini
\end{codefragment}

This sets up an environment variable which is persistent between sessions that tells happi where to load and save changes.

The Linux equivalent would be to create \nolinkurl{~/.config/happi.ini} with the contents of path pointing to \nolinkurl{~/.local/share/happi/happidb.json}.

Once set up, you can add all \texttt{yaq} hardware by doing:

\begin{codefragment}{bash}
yaqd list --format happi | happi update -
\end{codefragment}

This exports all of the daemons that are recognized by \texttt{yaqd-control} into the happi database.
If hardware is added to the system (or information like the host or port are updated) then the same line will update happi again.

\paragraph{Configure data folder}

You should create a data folder where \texttt{bluesky-in-a-box} will place the generated \texttt{wt5} files.
The exact location does not particular matter, but \nolinkurl{~/bluesky-cmds-data} is what will be used in this document.

\paragraph{Download \texttt{bluesky-in-a-box} Docker configuration}

Clone the git repository for \biab:

\begin{codefragment}{bash}
git clone https://github.com/wright-group/bluesky-in-a-box
\end{codefragment}

There are a few options for the Docker Compose files, a template for the file is provided in the root of the \texttt{bluesky-in-a-box} repository.
Copy the template file, \nolinkurl{.env-example} to \nolinkurl{.env}, then edit it.
\texttt{HAPPI\_DB\_PATH} should be set to the full path of \nolinkurl{happidb.json} as specified in \nolinkurl{happi.ini} file above.
\texttt{WT5\_DATA\_PATH} should be set to the full path of the data folder specified above.
Finally, \texttt{TZ} should be set to the appropriate timezone where it is running, for example \texttt{America/Chicago}.


\paragraph{Start and update containers}
Start the docker engine, either by opening Docker Desktop (Windows) or by enabling the background docker service (Linux).
Navigate to the \biab folder in a command prompt.
If needed, pull new versions or switch to appropriate branches that contain the version of \biab that you need.
To build for the first time or to perform a standard update, run the following:

\begin{codefragment}{bash}
docker compose up --build
\end{codefragment}

That will suffice for most cases, however if you want to force a full rebuild, you can use:

\begin{codefragment}{bash}
docker compose build --no-cache
\end{codefragment}

This is most useful as a development tool to ensure that you are not accidentally relying on the state of your local system in ways that would not work on other machines.

Once the containers are built, you can start them using Docker Desktop by pressing the start button for \biab in the ``Containers/Apps'' tab.
This will start the pre-built containers without rebuilding them.
Rebuilding using the CLI is needed whenever the containers themselves are changed, most commonly for bumping the versions of installed software.

% TODO longer term maintenance of docker containers and volumes

\subsubsection{\texttt{re-manager}}

The RE Manager is the beating heart of the \biab system.
It is the service which accepts parameterized plan descriptions, organizes them into a queue, allows manipulation of the queue itself, and, most importantly, actually conducts the experiment.
The RE Manager starts a Bluesky Run Engine (hence the name) and consumes the plans from the queue.
These plans encode the steps for taking data: moving motors, triggering sensors, and emitting Event Model documents.

The RE Manager is responsible for connecting to the devices which are listed by the HAPPI database, providing its own list of available devices (which filters out those that are offline or are subdevices of other hardware such as OPA motors).
At present, \biab expects that all hardware specified is \yaq hardware, though support for additional interface layers requires only that they are specified in HAPPI and that the necessary dependencies get added to the \texttt{re-manager} container.
It is also responsible for providing a list of available plans.
For \biab, all of the \wrightplans are available, as well as the built-in Bluesky \texttt{count}, \texttt{sleep}, and \texttt{mv} plans.
\texttt{count} generates a time series of readings, separated by a wait period.
\texttt{sleep} simply waits for a specified period of time, given in seconds.
\texttt{mv} allows for setting of hardware (to positions specified in their native units) in a manner which can be enqueued.
All of the plans which generate data are wrapped with the Bluesky \texttt{baseline} decorator, which reads \textit{all} movable hardware once before and once after the plan itself is consumed.
This allows for a full record of the state of the system without recording all hardware for every data point, even when it not told to move for a given plan.

The RE Manager container provides a few ways of interacting, which are largely based on intefaces over a ZeroMQ\cite{} message system.
ZeroMQ (abbreviated ZMQ or 0MQ) provides a standard way of either doing request/reply Remote Procedure Calls (RPCs) or publish/subscribe (pub/sub) data flows where one end provides a stream of data and the other is expecting that stream of data.
The former, RPC style requests, is used to enqueue plans, interact with the queue itself, and to query the state of the RE Manager.
The latter, pub/sub style requests, is used to publish the Event Model documents as well as (separately) publishing the console output of the RE Manager so that it can be displayed in client applications.
These ZMQ interfaces are considered trusted interfaces, and as such do no authentication of users.
If such authentication is needed, it can be provided by additional tools by the authors of \texttt{bluesky-queueserver} which wrap the ZMQ interface in a FastAPI\cite{} interface served over TCP, with authentication done by that service.
This is recommended in the future if remote access is desired to instruments runing \biab, however initial development is focused on running a self contained system which provides access primarily to the direct machine running the instrument.

\subsubsection{\texttt{zmq-proxy}}

\texttt{zmq-proxy} is a small program provide by Bluesky which simply routes Event Model documents that it reads from on port and writes them to any number of subscribers on a second port.
The same purpose in larger systems where caching, latency, and network reliability are important may be achived by an industry standard service like Apache Kafka\cite{}.
Because \biab is running on a single machine, network reliability and latency are not anticipated problem and we do not currently require caching, so the simpler program is used.
\texttt{zmq-proxy} provides a server for each end of the communication, which allows multiple clients to act as sources or sinks of the documents which are being shuttled.
In practice, usually only one service, the RE Manager, should ordinarily be the source of the documents.
However, this does provide an easy way for testing or special purpose scripts to serve as a source.

\subsubsection{\texttt{WT5}}

The WT5 container consumes the Event Model Document stream from the \texttt{zmq-proxy} container and creates WrightTools Data files.
These files are stored in a folder shared with the host platform, such that they can be browsed and opened using platform native tools.
In addition, the WT5 container writes a number of text files into the data folder and generates images for many scans.
These include the ``start'' ``descriptor'' and ``stop'' documents from the event model for each Bluesky Run, written as JSON text files.
Additionally each generated WT5 file also writes the output of \texttt{data.print\_tree(verbose=True)} to a file so that users can quickly see information such as the shape and axes of the data file without actually opening the file.

One and Two dimensional scans produce the ``quick'' plots provided by WrightTools.
Larger scans have caused significant memory problems when attempting to plot many slices, and so have been disabled.

\subsubsection{\texttt{mongo}}

MongoDB\cite{} is an industry standard database for storing JSON-like documents.
Mongo is widely used by the upstream authors of Bluesky at various National Labs as the immediate store of data being recorded from instruments.
As such, there are pre-existing tools in the ecosystem that pair well with a Mongo database.
The Mongo Docker container is provided to allow exploration of this ecosystem of tools with the data collected by \biab.
It also provides a back-up of the data should the WT5 data writer fail.
At present, the Mongo database does not serve as a long term archival data back-up, but rather as a a short term failsafe.
This decision is not one of fundemental design, but rather a decision of allocation of development time.
It is quick and easy to add a Mongo container to the Docker system and route the data from the RE Manager to it, ensuring it is accessible short term.
It is a more complex task to ensure that the data gets written in a manner appropriate for archival storage, and there has been no need for it at this time.

\subsubsection{\texttt{redis}}

Redis\cite{} is an industry standard service for in-memory key-value data store.
It is used by the RE Manager container to track the state of the queue of plans.
Redis is required for the operation of the RE Manager, and thus must be running.
However, there is little to no direct interaction with redis, so unless you are actively developing \texttt{bluesky-queueserver}, it can largely be ignored.

\subsubsection{\texttt{hwproxy}}

\texttt{bluesky-hwproxy}\cite{} is a small project started for inclusion in \biab.
Its primary purpose is to provide read-only access to the hardware which is available to the RE Manager.
This allows for rich graphical programs like \texttt{bluesky-cmds} to query information about hardware such as limits and units.
\texttt{hwproxy} wraps the device loading portions of \texttt{bluesky-queueserver} and exposes the standard Bluesky Protocol methods over a ZMQ interface very similar to that used by the RE Manager itself.
This allows any tool that can communicate with the RE Manager to communicate with \texttt{hwproxy} just as easily.

\subsubsection{Troubleshooting}

Docker as a technology for running \biab was chosen primarily to make standard usage as easy as possible.
It allows for all of the connected services to be brought online and offline quickly and easily, encoding all of their dependent relationships on each other.
However, the same factors that make Docker a good choice for standard operation mean that a few extra steps are needed when troubleshooting or debugging.
In particular, Docker is designed to provide a consistent environment for all of the services to run, with static code and dependencies.
Troubleshooting is a task which necessitates rapid updating and a tight feedback loop.

While the standard troubleshooting tips as laid out in \ref{TODO} apply, there are some particular approaches that are useful when debugging \biab in particular (and Docker containers more generally).
The first step is to determine \textit{where} the origin of the problem exists.
This is not as trivial as it may seem at first.
Generally, it is a good strategy to start by carefully reading the logs provided by the Docker containers.
The two most relevant containers will be the the RE Manager and the WT5 containers.
The other containers are all fairly standard tools that are not nearly as customized and thus are not likely to be the problem.
Most of the time, the container which has an error message is a good candidate for where the problem exists, but this is not always the case.
Some problems may exist in the network itself which connects the containers to each other and to the host system.
These tend to be relatively rare, but can also be some of the trickiest to track down and resolve.
Others may present as errors down stream but the root cause is the input to that container.
For example, faulty parameters may cause a plan to fail during execution.
Such a failure may be expected, given the inputs, so this example means that either user training or user interfaces could be improved.
Alternatively, if the metadata is malformed that may allow the actual data acquisition to proceed as expected, but cause the WT5 data writer to fail.
In this instance, the root cause would be the Bluesky plan, which is a dependency of the RE Manager, though the error would present in the WT5 container.


For smaller tests, particularly those involving testing of the network connections, Docker allows opening a command shell which runs inside of the container.
If using Docker Desktop, starting a command shell inside of one of the Docker containers is as easy as pressing a button in the graphical application.
On any platform, you can use the Docker command line to open a shell in the Docker container:

\begin{codefragment}{bash}
docker exec -it <container name> bash
\end{codefragment}

Note that the container name here is as listed by \inlinebash{docker ps}, not the name in the Docker Compose file.
For \biab, this is most useful for information gathering tasks such as answering the question "Can the RE Manager container communicate with the yaq daemon running on the host machine?" or "Can my WT5 container communicate with the ZMQ Proxy container?".

If the problem exists with the code that is in the \biab repository (mostly the startup scripts, configuration, and the WT5 data writing code) then a more traditional editing the code and running the containers is reasonably achievable.
There is the required step to do \inlinebash{docker compose up --build} to run the Docker containers.
Without doing this, the old version will run instead.
This makes for a reasonable, if a little slower than ideal, development feedback loop.

If, instead, the problem exists in your dependencies, that makes developing even more indirect.
There is not a particularly good way to build the Docker containers pointing to local directories with code changes.
Thus the procedure is instead to push the code changes to a remotely accessible Git repository and install Python dependencies from that Git repository.
Note, however, that while you \textit{can} point the Git installation in \texttt{requirements.txt} to the branch that you create, this is not fully recommended because subsequent updates to the branch will not cause the Docker container to rebuild.
Docker will only redo a step in the installation if it can tell that the step requires it.
Thus it will only redo the step that installs the Python dependencies if it sees a change to what it is asked to install.
Since the requested branch is the same, the Docker build system determines that it is the same version being requested and thus will use the previously built version.
Instead, it is recommended to use the SHA hash pointing to a particular commit instead, and updating it every time the code is pushed to the remote repository.

An example of a \texttt{requirements.txt} that points to a Git repository:

\begin{codefragment}{python}
bluesky==1.8.3
WrightTools==3.4.4
pyzmq==22.3.0
numpy==1.22.3
toolz==0.11.2
git+https://github.com/wright-group/WrightTools@abcdef12
\end{codefragment}

The last line adds a particular Git version of \wrighttools which overrides the version specified earlier in the requirements file.

Finally, and perhaps most powerfully, the modularity of the Docker system can be used to run parts of the system outside of docker itself, thus enabling more traditional development and feedback cycles.
Any of the Docker containers can be independently halted and superceded by a version of the service running outside of Docker, though clients of that service may need to be reconfigured to point to the correct external service.
While it can be used if there are problems discovered with the MongoDB, Redis, or ZMQ Proxy contianers, for \biab this is most useful to perform tests using a smaller Run Engine running outside of the RE Manager container all together.
\ref{apx:script:re} provides an example of a script that can be run directly on the host machine, performing data acquisitions which are still processed by the WT5 data writing pipeline (via the ZMQ Proxy container).
This particular script does not require \textit{any} change to the configuration of \biab.
In fact, even the RE Manager container can be running, though it should not be actively acquiring data, as downstream tools such as the WT5 writer and live plotting are likely to behave poorly if there are two simultaneous acquisitions happening.
This script allows for a much smaller and easier to troubleshoot system to do the actual orchestration of the acquisition.
It removes some of the complexities of of the RE Manager container, allowing for more direct access to the Run Engine.
This comes, of course, at the cost of not having a queue or the user interfaces built to interact with the RE Manager.

% Slow and methodical.

% Do not be afraid to try things; take precautions such as physically blocking laser light if needed

% Examine and think carefully about expected results; do parameters make sense

% Seek a reproducible test case

% Find cases which work as expected as well as those which fail unexpectedly


\subsection{\texttt{qserver}: Command Line Frontend to \texttt{bluesky-queueserver}}

\subsubsection{Installation}

\texttt{qserver} is included with the installation of the Python package \texttt{bluesky-queueserver}.
As such it is installed via:

\begin{codefragment}{bash}
conda install bluesky-queueserver
\end{codefragment}

or:

\begin{codefragment}{bash}
pip install bluesky-queueserver
\end{codefragment}

\subsection{\texttt{bluesky-cmds}: Graphical Frontend to \texttt{bluesky-queueserver}}

\subsubsection{Installation}

\texttt{bluesky-cmds} is included with the installation of the Python package \texttt{bluesky-cmds}.
However, the package is not yet available on conda, so conda systems should install the dependencies via conda first.
As such it is installed via:

\begin{codefragment}{bash}
conda install bluesky-queueserver-api appdirs click pyqtgraph pyside2 qtpy toml toolz wrighttools sympy bluesky-widgets
pip install bluesky-hwproxy bluesky-cmds --no-deps
\end{codefragment}

or:

\begin{codefragment}{bash}
pip install bluesky-cmds
\end{codefragment}

or, for development purposes (conda users should still install dependencies as above):

\begin{codefragment}{bash}
git clone https://github.com/wright-group/bluesky-cmds
cd bluesky-cmds
flit install --pth-file
\end{codefragment}

\subsubsection{Configuration}

If running locally, then the default configuration options should suffice.
To populate or edit the configuration file you can execute:

\begin{codefragment}{bash}
bluesky-cmds edit-config
\end{codefragment}

This will prompt you to populate the file with the default template if it does not already exist, and open the configuration file in a text editor.

The default template, appropriate for connecting to a \biab instance running on the local machine is shown below:

\begin{codefragment}{toml}
test=true # needed for minted TODO figure this out
[bluesky]
re-manager = "tcp://localhost:60615"
re-info = "tcp://localhost:60625"
hwproxy = "tcp://localhost:60620"
zmq-proxy = "localhost:5568"
\end{codefragment}

To run remotely, the host and port for each connected service can change as needed.

\subsection{\texttt{wright-fakes}: Simulated Hardware for Development}
\label{wright-fakes}

\wrightfakes is a collection of docker containers each running a single \yaq daemon.
\wrightfakes provides an entire system, mimicking a laser table, but using only fake daemons to represent motors.
It includes detectors, a monochromator, delays, neutral density wheels, and OPAs (including the component motors of the OPAs and delays).

This serves as a quick and easy way to start a suite of \yaq daemons which is consistent and provides the variety of hardware needed to thoroughly test the data acquisition software.

\subsubsection{Installation}
These instructions assume that Docker is already installed, for more information see the \ref{biab-install}.


Clone the git repository for \wrightfakes:

\begin{codefragment}{bash}
git clone https://github.com/wright-group/wright-fakes
\end{codefragment}

Inside of this repository there are folders for each available system (at time of writing, this is only \texttt{fs}).
Navigate in a command prompt to the folder for the system you wish to start and run:

\begin{codefragment}{bash}
docker compose up --build
\end{codefragment}

This builds the system of simulated daemons and starts running the daemons through docker.
If the port that is expected for a \yaq daemon is already in use, the container will fail to run, much the same as if you tried to run a second instance of a \yaq daemon


If you are using Docker Desktop, then you can restart the fake daemons from the ``Containers/Apps'' tab of the Docker Desktop application, in the same manner as \biab.

Once installed, you need to update the \texttt{yaqd-control} cache so that it knows about all of the daemons:

\begin{codefragment}{bash}
yaqd scan
\end{codefragment}

On Windows, this can be a long process, so you may wish to search only the ranges where actual daemons are running

\begin{codefragment}{bash}
yaqd scan --start=38001 --stop=38003
yaqd scan --start=38401 --stop=38403
yaqd scan --start=38451 --stop=38453
yaqd scan --start=38500 --stop=38501
yaqd scan --start=38550 --stop=38551
yaqd scan --start=38999 --stop=39000
yaqd scan --start=39301 --stop=39303
yaqd scan --start=39501 --stop=39508
yaqd scan --start=39601 --stop=39608
yaqd scan --start=39701 --stop=39703
yaqd scan --start=39876 --stop=39877
\end{codefragment}

\subsection{Bluesky Community and Ecosystem}
\subsubsection{\texttt{tiled}}
\subsubsection{\texttt{databroker}}

\clearpage
